# Variational Inference with Normalizing Flows

# Goal

### Proposing a simple approach for learning highly non-Gaussian posterior densities by learning transformations of simple densities to more complex ones through a **normalizing flow**

# Motivations

![Untitled](assets/img/2023-11-16-write-Variational Inference_with_Normalizing_Flows/Untitled.png)

**Variational Inference**ì˜ ëª©ì ì€ ê³„ì‚°ì´ ì–´ë ¤ìš´ ì‚¬í›„í™•ë¥  ë¶„í¬ $p(z|x)$ë¥¼ ê³„ì‚°ì´ ë³´ë‹¤ ì‰¬ìš´ approximate posterior distributionì¸ $q(z|x)$ë¡œ ê·¼ì‚¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ìš°ë¦¬ëŠ” *evidence*ì¸ $ğ‘(ğ‘¥)$ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ëª¨ë¸, ì¦‰ ë°ì´í„° $ğ‘¥$ì˜ ë¶„í¬ë¥¼ ì˜ ì„¤ëª…í•˜ëŠ” í™•ë¥ ëª¨í˜•ì„ í•™ìŠµì‹œí‚¤ê³ ì í•©ë‹ˆë‹¤.

ì´ë•Œ **approximate posterior distribution**ì˜ ì„ íƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

ê·¼ë° ëŒ€ë¶€ë¶„ì˜ ê²½ìš°, approximate posteriorì„ ìƒ˜í”Œë§ê³¼ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•˜ê¸° ìœ„í•´ì„œ ë‹¨ìˆœí•œ distributionì„ ì‚¬ìš©í•˜ê²Œ ë©ë‹ˆë‹¤. (usually it is a multivariate Gaussian)

Variational inferenceì— simple distributionì„ í™œìš©í•˜ëŠ” ê²½ìš°, inference ì„±ëŠ¥ì´ ë–¨ì–´ì§€ê³  í‘œí˜„ë ¥ì´ ë¶€ì¡±í•˜ë‹¤ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.

Richer, more faithful posterior approximationsëŠ” ë” ì¢‹ì€ performanceë¥¼ ë³´ì—¬ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” simple distribution ëŒ€ì‹  **Normalizing flows**ë¥¼ í™œìš©í•˜ì—¬ rich posterior approximationsì„ ë§Œë“¤ê³  ì´ë¥¼ **Variational inference**ì— í™œìš©í•˜ê² ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

# Amortized Variational Inference

Variational inferenceì—ì„œëŠ” ì‚¬í›„í™•ë¥ ì— ê·¼ì‚¬í•œ $ğ‘(ğ‘§)$ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ Kullback-Leibler divergenceì„ í™œìš©í•©ë‹ˆë‹¤.

## **KL Divergence (KLD)**

<aside>
ğŸ’¡ **KL divergence**

KL divergenceëŠ” non-symmetricí•˜ê²Œ ë‘ ê°œì˜ í™•ë¥  ë¶„í¬ $P$ì™€ $Q$ì‚¬ì´ì˜ differenceë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²•ì„

ì—¬ê¸°ì„œ ë³´í†µ $P$ëŠ” the true posterior distribution, $Q$ëŠ” the approximate distributionë¼ëŠ” ê°€ì •ì„ ì‚¬ìš©í•œë‹¤

![Untitled](assets/img/2023-11-16-write-Variational Inference_with_Normalizing_Flows/Untitled%201.png)

discreteì™€ continuous í™•ë¥  ë¶„í¬ì— ëŒ€í•´ ê°ê° ì´ë ‡ê²Œ ì •ì˜í•  ìˆ˜ ìˆê³ ,

information entropyë¡œ KL divergenceë¥¼ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆìŒ

**KL divergence as information Gain**

EntropyëŠ” ê°„ë‹¨íˆ ë§í•˜ë©´, the average amount of informationì´ë‹¤

ë˜í•œ ë¶ˆí™•ì‹¤ì„±(uncertainty)ì— ëŒ€í•œ ì²™ë„ë¼ê³  í•  ìˆ˜ ìˆìŒ

- ì˜ˆì‹œë¥¼ í†µí•œ **Entropy**ì— ëŒ€í•œ ì§ê´€ì ì¸ ì´í•´
    
    ë§Œì•½ ì–´ë–¤ ê°€ë°© ì•ˆì— ë¹¨ê°„ ê³µë§Œ ë“¤ì–´ìˆë‹¤ë©´, ì–´ë–¤ ê³µì„ êº¼ë‚´ë„ ë¹¨ê°„ ê³µì¸ ê²ƒì„ ì´ë¯¸ ì•Œê³  ìˆê¸° ë•Œë¬¸ì— ë¶ˆí™•ì‹¤ì„±ì€ ì—†ë‹¤ê³  í•  ìˆ˜ ìˆê³  ìš°ë¦¬ê°€ ì–»ì„ informationì´ ì ë‹¤ê³  í•  ìˆ˜ ìˆìŒ (EntropyëŠ” 0)
    
    ë§Œì•½ ë¹¨ê°„ ê³µê³¼ ì´ˆë¡ ê³µì´ ë°˜ë°˜ ë“¤ì–´ ìˆë‹¤ë©´, ì–´ë–¤ ê³µì´ ë” ìì£¼ ê´€ì°°ë  ì§€ ëª¨ë¥´ê¸° ë•Œë¬¸ì— ë¶ˆí™•ì‹¤ì„±ì´ ê°€ì¥ í¬ë‹¤ê³  í•  ìˆ˜ ìˆê³  ê³µì„ êº¼ë‚¼ ë•Œ ìš°ë¦¬ê°€ ì–»ì„ informationì´ ë§ë‹¤ (Entropyê°€ ê°€ì¥ í¼)
    
    ì¦‰, EntropyëŠ” ì˜ˆì¸¡í•˜ê¸° ì‰¬ìš´ ì¼ì—ì„œ ë³´ë‹¤, ì˜ˆì¸¡í•˜ê¸° í˜ë“  ì¼ì—ì„œ ë” ë†’ë‹¤
    
    ì˜ˆì¸¡í•˜ê¸° ì‰¬ìš´ ì¼ì—ì„œëŠ” ìš°ë¦¬ê°€ ì–»ì„ ì •ë³´ëŸ‰ì€ ì ê³  ì˜ˆì¸¡í•˜ê¸° ì–´ë ¤ìš´ ì¼ì—ì„œëŠ” ìš°ë¦¬ê°€ ìƒˆë¡­ê²Œ ì–»ì„ ì •ë³´ëŸ‰ì€ ë§ë‹¤ê³  ì´í•´í•  ìˆ˜ ìˆë‹¤
    
    ê·¸ë˜ì„œ EntropyëŠ” ë¶ˆí™•ì‹¤ì„±ì— ëŒ€í•œ ì²™ë„ì´ì informationì˜ averageë¼ê³  í•˜ëŠ” ê²ƒ
    

EntropyëŠ” discreteì™€ continuous í™•ë¥  ë¶„í¬ì— ëŒ€í•´ ì•„ë˜ì™€ ê°™ì´ ì •ì˜ ê°€ëŠ¥í•œë°,

![Untitled](assets/img/2023-11-16-write-Variational Inference_with_Normalizing_Flows/Untitled%202.png)

- **Entropy**ë¥¼ the minimum number of bits(or symbol) you need to encode an event drawn from your probability distributionìœ¼ë¡œë„ ì§ê´€ì ìœ¼ë¡œ ì´í•´ ê°€ëŠ¥
    
    Entropyë¥¼ probability distributionì˜ ì‚¬ê±´(or symbol)ì„ encodingí•˜ëŠ”ë° í•„ìš”í•œ ìµœì†Œí•œì˜ bit ìˆ˜ë¡œë„ ì´í•´ë¥¼ í•  ìˆ˜ ìˆë‹¤ (seeÂ [Shannon's source coding theorem](https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem))
    
    ì˜ˆë¥¼ ë“¤ì–´, fair eight-sided ì£¼ì‚¬ìœ„ê°€ ìˆìœ¼ë©´ ê° ê²°ê³¼ëŠ” equi-probable
    
    ê·¸ë˜ì„œ ì£¼ì‚¬ìœ„ì˜ ê²°ê³¼ë¥¼ encodingí•˜ëŠ”ë° í•„ìš”í•œ bitì˜ ìˆ˜ëŠ” í‰ê· ì ìœ¼ë¡œ
    
     $`\sum_1^8 -\frac{1}{8}log_2(\frac{1}{8}) = 3`$ 3ê°œê°€ ë¨
    
    ë§Œì•½ì— ì£¼ì‚¬ìœ„ê°€ a weighted eight-sided ì£¼ì‚¬ìœ„ë¼ë©´, (8ì´ ë‚˜ì˜¬ í™•ë¥ ì´ ë‹¤ë¥¸ ìˆ˜ë³´ë‹¤ 40ë°° ë†’ìŒ)
    
    í‰ê· ì ìœ¼ë¡œ, ì£¼ì‚¬ìœ„ì˜ ê²°ê³¼ë¥¼ encodingí•˜ëŠ”ë° 1ê°œì˜ bitê°€ ë” í•„ìš”í•¨
    
    (to get close, we would assign "8" to a single bitÂ 0, and others to something likeÂ 10,Â 110,Â 111Â ... using aÂ [prefix code](https://en.wikipedia.org/wiki/Prefix_code))
    
    ì´ëŸ¬í•œ ê´€ì ì—ì„œ **Entropy**ë¥¼ ì´í•´í•œë‹¤ë©´, 
    
    theoretical average message lengthì— ìµœëŒ€í•œ ê°€ê¹ë„ë¡ symbolì„ í™•ë¥  ë¶„í¬ $P$ì—ì„œ ê°€ì ¸ì˜¤ê³  ìˆë‹¤ëŠ” ê°€ì •ì„ ì‚¬ìš©í•˜ê³  ìˆëŠ” ê²ƒì´ë‹¤
    
    ë‹¤ë¥¸ ë¶„í¬ì¸ $Q$ì˜ ideal symbolì„ ì‚¬ìš©í•œë‹¤ë©´ average message length(i. e. **Entropy**)ëŠ” ì–´ë–»ê²Œ ë ê¹Œ? $P$ì™€ $Q$ì˜ **Cross Entropy**ê°€ ë  ê²ƒì„
    
    $ğ»(ğ‘ƒ,ğ‘„):=ğ¸_ğ‘ƒ[ğ¼_ğ‘„(ğ‘‹)]=ğ¸_ğ‘ƒ[âˆ’log(ğ‘„(ğ‘‹))]$
    
    ë‹¹ì—°íˆ ideal encodingë³´ë‹¤ í´ ê²ƒì´ê³  average message lengthê°€ ì¦ê°€í•¨
    
    ì¦‰, $Q$ì˜ codeë¥¼ ì‚¬ìš©í•˜ë©´ì„œ ë¶„í¬ $P$ì˜ message(ì‚¬ê±´)ì„ transmití•  ë•Œ, ë” ë§ì€ ì •ë³´(or bits)ê°€ í•„ìš”í•˜ë‹¤ëŠ” ë§ì´ë‹¤
    
    (ì•ì—ì„œ true posterior distributionì„ approximate distributionìœ¼ë¡œ ì¶”ì •í•  ë•Œ ìƒê¸°ëŠ” ì°¨ì´ì™€ ê°™ì€ ë§¥ë½ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆìŒ)
    

ì¦‰, **KL divergence**ë¥¼ ìš°ë¦¬ê°€ í™•ë¥ ë¶„í¬ $P$ ëŒ€ì‹  í™•ë¥ ë¶„í¬ $Q$ë¥¼ ì‚¬ìš©í•˜ë©´ì„œ í™•ë¥  ë¶„í¬ë¥¼ ì˜ëª» ì¶”ì •í•  ë•Œ í•„ìš”í•œ **average extra-message length**ë¡œ ë³¼ ìˆ˜ ìˆë‹¤

![Untitled](assets/img/2023-11-16-write-Variational Inference_with_Normalizing_Flows/Untitled%203.png)

ê·¸ë˜ì„œ ë§Œì•½ theoretic minimal distributionì¸ $P$ê°€ ìˆì„ë•Œ,

approximation distributionì¸ Që¥¼ ì°¾ì•„ì„œ KL divergenceë¥¼ ìµœì†Œí™”í•˜ì—¬

Pì— ê°€ê¹Œìš´ ë¶„í¬ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.

**Forward and Reverse KL Divergence**

KL divergenceì—ì„œ ì£¼ëª©í•  ì ì€ not symmetricí•˜ë‹¤ëŠ” ê²ƒì´ë‹¤

$ğ·_{ğ¾ğ¿}(ğ‘ƒ||ğ‘„)â‰ ğ·_{ğ¾ğ¿}(ğ‘„||ğ‘ƒ)$

ì¢Œë³€ì„ **Forward KL divergence**, ìš°ë³€ì„ **Reverse KL divergence**ë¼ê³  í•¨

1. **Forward KL divergence**
ì‹ê³¼ ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, $P$ is large and $Q â†’ 0$ ì¼ë•Œ, KL divergenceëŠ” ë°œì‚°í•˜ê²Œ ë¨
ê·¸ë˜ì„œ ì ì ˆí•œ approximate distribution $Q$ë¥¼ ê³ ë¥¼ ë•Œ, ìµœëŒ€í•œ $P$ì˜ non-zero partë¥¼ coverí•˜ë„ë¡ $Q$ë¥¼ ì„ íƒí•˜ê²Œ ëœë‹¤ (ê·¸ë¦¼ì—ì„œ $P$ëŠ” multimodalì´ì§€ë§Œ $Q$ëŠ” bell shapedì„)
ì´ë•Œ ë¬¸ì œì ì€ Forward KL divergenceë¥¼ ìµœì†Œí™”í•˜ë©´ì„œ, original distributionì—ì„œëŠ” low densityë¥¼ ê°€ì§€ì§€ë§Œ, approximate distributionì—ì„œëŠ” maximum densityë¥¼ ê°€ì§„ë‹¤ëŠ” ì ì„ (center of $Q$)
    
    ![Untitled](assets/img/2023-11-16-write-Variational Inference_with_Normalizing_Flows/Untitled%204.png)
    
2. **Reverse KL divergence**
ì—¬ê¸°ì„œë„ ë§ˆì°¬ê°€ì§€ë¡œ $P$ê°€ theoretic distribution, $Q$ê°€ approximation
ì‹ê³¼ ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, $P$ is small and $Q$ is not smallì´ë©´ ë°œì‚°í•˜ê²Œ ë¨
ì¶”ê°€ì ìœ¼ë¡œ, $P$ is largeë©´ ì•„ë¬´ ë¬¸ì œì—†ìŒ
ê·¸ë¦¼ì„ ë³´ë©´, $Q$ì˜ tailìª½ì—ì„œ í›¨ì”¬ ë¹ ë¥´ê²Œ drop offë¥¼ í•˜ê¸° ë•Œë¬¸ì— ë°œì‚°í•˜ê²Œ ë˜ëŠ” ë¬¸ì œê°€ ìƒê¸´ë‹¤
ì ì ˆí•œ approximate distribution $Q$ë¥¼ ê³ ë¥¼ë•Œ, $P$ì™€ $Q$ì˜ tailì´ ë¹„ìŠ·í•œ rateë¡œ drop offí•˜ë„ë¡ Që¥¼ ì„ íƒí•˜ê²Œ ëœë‹¤
ê·¸ë˜ì„œ Reverse KL divergenceë¥¼ ìµœì†Œí™”í•˜ë©´ì„œ, ê·¸ë¦¼ì—ì„œë„ ë³¼ ìˆ˜ ìˆë“¯ì´,
QëŠ” ë¶„í¬ Pì˜ mode ì¤‘ í•˜ë‚˜ì— ì˜ matchingí•˜ê³  ìˆê³  ì¢‹ì€ approximationì„ í•˜ê³  ìˆë‹¤
**Reverse KL divergenceë¥¼ ìì£¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ** ì¤‘ í•˜ë‚˜ì´ê³  ë‹¤ë¥¸ ìˆ˜í•™ì ì¸ ì´ìœ ë„ ìˆë‹¤ê³  í•œë‹¤
    
    ![Untitled](assets/img/2023-11-16-write-Variational Inference_with_Normalizing_Flows/Untitled%205.png)
    
    ![Untitled](assets/img/2023-11-16-write-Variational Inference_with_Normalizing_Flows/Untitled%206.png)
    
</aside>

ì‚¬í›„í™•ë¥  ë¶„í¬Â p(z|x)ì™€ $ğ‘(ğ‘§)$Â ì‚¬ì´ì˜ KLDë¥¼ ê³„ì‚°í•˜ê³ , KLDê°€ ì¤„ì–´ë“œëŠ” ìª½ìœ¼ë¡œÂ $ğ‘(ğ‘§)$ë¥¼ ì¡°ê¸ˆì”© ì—…ë°ì´íŠ¸í•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•˜ë©´ ì‚¬í›„í™•ë¥ ì„ ì˜ ê·¼ì‚¬í•˜ëŠ”Â $ğ‘^*(ğ‘§)$ë¥¼ ì–»ê²Œ ë  ê²ƒì´ë¼ëŠ” ê²Œ Variational inferenceì˜ í•µì‹¬ ì•„ì´ë””ì–´ì…ë‹ˆë‹¤. 

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” í™•ë¥  ëª¨ë¸ì˜ ì£¼ë³€ ê°€ëŠ¥ì„±(marginal likelihood)ì„ ì‚¬ìš©í•˜ì—¬ inferenceë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤.

ì´ë¥¼ ìœ„í•´ì„œëŠ” ëª¨ë¸ì˜ ì ì¬ì ì¸ ë³€ìˆ˜(latent variables)ë“¤ì„ marginalizeí•˜ëŠ” ê²ƒì´ í•„ìš”í•©ë‹ˆë‹¤. 

ì´ ê³¼ì •ì€ ì¼ë°˜ì ìœ¼ë¡œ ì‹¤í–‰ ë¶ˆê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì—, ëŒ€ì‹  ì£¼ë³€ ê°€ëŠ¥ì„±(marginal probability)ì˜ í•˜í•œ(lower bound)ì„ ìµœì í™”í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

![Untitled](assets/img/2023-11-16-write-Variational Inference_with_Normalizing_Flows/Untitled%207.png)

ëª¨ë¸ì€ ê´€ì¸¡ê°’(observed data) $*x*$, ì ì¬ ë³€ìˆ˜(latent variable) $*z$,* ê·¸ë¦¬ê³  ëª¨ë¸ íŒŒë¼ë¯¸í„° $*Î¸*$ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

$*p_Î¸(xâˆ£z)*$ëŠ” likelihoodì´ê³  $*p(z)*$ëŠ” ì ì¬ ë³€ìˆ˜(latent variable)ì— ëŒ€í•œ ì‚¬ì „ ë¶„í¬(prior distribution)ì…ë‹ˆë‹¤.

ì  ìŠ¨ì˜ ë¶€ë“±ì‹(Jensen's inequality)ì„ í†µí•´ ìµœì¢… í•˜í•œ(Lower Bound)ì´ ë„ì¶œë˜ëŠ”ë°, ìš°ë¦¬ëŠ” ì´ ë§ˆì§€ë§‰ ë¶€ë“±ì‹ì˜ ìš°ë³€ì„Â **Evidence lower bound, ì¤„ì—¬ì„œ ELBO**ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤. 

ì´ ELBOë¥¼ maximizeí•¨ìœ¼ë¡œì¨ likelihood ë˜í•œ maximizeí•  ìˆ˜ ìˆëŠ” ê²ƒì…ë‹ˆë‹¤. 

ì´ ELBOë¥¼ ì¢€ ë” ìì„¸íˆ ì‚´í´ë³´ë©´,Â 

**ì²« ë²ˆì§¸ í•­ì€ approximate posterior $q(z|x)$ì™€ prior $p(z)$ ê°„ì˜ KL divergence**ì™€ ê°™ìŠµë‹ˆë‹¤. 

ê·¸ë ‡ê¸° ë•Œë¬¸ì— ì´ëŠ” **approximate posterior**ì™€ **prior**ê°€ ìµœëŒ€í•œ ë¹„ìŠ·í•˜ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” **error**ë¼ê³  í•  ìˆ˜ ìˆê³ , ì´ëŠ” VAEê°€ reconstruction taskë§Œ ì˜ í•˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ë•Œë¬¸ì— **Regularization error**ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.

**ë‘ë²ˆì§¸ í•­ì€ $p(x|z)$ì™€ $q(z|x)$ì‚¬ì´ì˜ negative cross entropyì™€** ê°™ìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ë•Œë¬¸ì— ì´ëŠ” Encoderì™€ Decoderê°€ Auto-encoderì²˜ëŸ¼ reconstructionì„ ì˜ í•  ìˆ˜ ìˆê²Œ ë§Œë“¤ì–´ì£¼ëŠ” errorë¼ê³  í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— **Reconstruction error**ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.

ìœ„ ê³¼ì •ì„ í†µí•´,Â $*log p_{(\theta)}(x)*$ì— ëŒ€í•´ lower boundë¥¼ ì–»ì–´ëƒˆìŠµë‹ˆë‹¤.

ì´ì œ $q(z|x)$ë¥¼ Normalizing Flowë¥¼ í†µí•´ ë³µì¡í•œ approximate posterior distributionìœ¼ë¡œ ë§Œë“¤ê³  **ELBO**ë¥¼ ìµœëŒ€í™”í•œë‹¤ë©´ Â $*log p_{(\theta)}(x)*$ë¥¼ ì˜ ê·¼ì‚¬í–ˆë‹¤ëŠ” ê²°ë¡ ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## **Stochastic Backpropagation**

ë¯¸ë‹ˆë°°ì¹˜(mini-batches)ì™€ í™•ë¥ ì  ê·¸ë˜ë””ì–¸íŠ¸ í•˜ê°•ë²•(stochastic gradient descent)ì„ ì‚¬ìš©í•˜ì—¬, ë§¤ìš° í° ë°ì´í„° ì…‹ì— Variational Inferenceë¥¼ ì ìš©í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. 

ì´ë¥¼ ìœ„í•´ ë‘ ê°€ì§€ ì£¼ìš” ë¬¸ì œë¥¼ í•´ê²°í•´ì•¼ í•˜ëŠ”ë°,

1) expected log-likelihoodì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°

2) ê³„ì‚°ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê°€ì¥ í’ë¶€í•œ ê·¼ì‚¬ ì‚¬í›„ ë¶„í¬(Approximate posterior)ì˜ ì„ íƒ

ì…ë‹ˆë‹¤.

1) expected log-likelihoodë¥¼ ìœ„í•´ì„œ stochastic backpropagationì„ ì§„í–‰í•˜ëŠ”ë°, two stepìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤.

1. **Reparameterization**
    
    $*q_Ï•(z)$ê°€ $N(z|Î¼,Ïƒ^2)$*ì¸ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¼ë©´, í‘œì¤€ ì •ê·œ ë¶„í¬ë¥¼ ê¸°ë°˜ ë¶„í¬ë¡œ ì‚¬ìš©í•˜ì—¬ *z*ë¥¼
    
    $*z = Î¼ + ÏƒÏµ, Ïµâˆ¼N(0,1)*$)ìœ¼ë¡œ ì¬ë§¤ê°œë³€ìˆ˜í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    ì¦‰, ì ì¬ ë³€ìˆ˜(latent variable)ë¥¼ ì•Œê³ ìˆëŠ” ë¶„í¬ì™€ ë¯¸ë¶„ ê°€ëŠ¥í•œ ë³€í™˜ì„ ì‚¬ìš©í•˜ì—¬ ì¬ë§¤ê°œë³€ìˆ˜í™”ë¥¼ í†µí•´ ë¯¸ë¶„ ê°€ëŠ¥í•œ ë°©ì‹ìœ¼ë¡œ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
    
2. **Backpropagation with Monte Carlo**
    
    ![Untitled](assets/img/2023-11-16-write-Variational Inference_with_Normalizing_Flows/Untitled%208.png)
    

í™•ë¥ ì ì¸ ìƒ˜í”Œë§ì„ ê¸°ë°˜ìœ¼ë¡œ ê¸°ìš¸ê¸°ì˜ ê¸°ëŒ€ì¹˜ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤. 

ê¸°ë³¸ ë¶„í¬ë¡œë¶€í„° ì—¬ëŸ¬ ìƒ˜í”Œì„ ì¶”ì¶œí•˜ê³ , ì´ë“¤ì„ í†µí•´ log-likelihoodì˜ ê¸°ìš¸ê¸°ë¥¼ ì¶”ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

# Normalizing Flow

**Normalizing Flow**ëŠ” invertible mappingì˜ seriesë¥¼ í†µí•´ì„œ ë‹¨ìˆœí•œ probability densityë¥¼ transformingí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ complex distributionì„ ë§Œë“¤ ìˆ˜ ìˆëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

Normalizing Flowë¥¼ ì´í•´í•˜ê¸° ì „, ì•Œì•„ì•¼ í•  ê°œë… ë¨¼ì € ì†Œê°œí•˜ê² ìŠµë‹ˆë‹¤.

## Change of Variable Theorem

ì–´ë– í•œ ë³€ìˆ˜ ë˜ëŠ” ë‹¤ë³€ìˆ˜ë¡œ ë‚˜íƒ€ë‚¸ ì‹ì„ ë‹¤ë¥¸ ë³€ìˆ˜ ë˜ëŠ” ë‹¤ë³€ìˆ˜ë¡œ ë°”ê¿” ë‚˜íƒ€ë‚´ëŠ” ê²ƒì„ **[Change of Variable](https://en.wikipedia.org/wiki/Change_of_variables)**ì´ë¼ê³  í•©ë‹ˆë‹¤. 

ì˜ˆì‹œë¥¼ ë“¤ë©´, ì–´ë– í•œ ëœë¤ ë³€ìˆ˜ $x$ê°€ ìˆê³ , $x$ì— ëŒ€í•œ **í™•ë¥  ë°€ë„ í•¨ìˆ˜(Probability Density Function)**ê°€ ìˆë‹¤ê³  ê°€ì •í•˜ê² ìŠµë‹ˆë‹¤.

$x \sim p(x) \\
z\sim \pi(z)$

ì—¬ê¸°ì„œ **ë³€ìˆ˜ $z$ê°€ ë³€ìˆ˜ $x$ë¥¼ ì˜ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ì ì¬ ë³€ìˆ˜(latent variable)**ì´ê³  $z$ì˜ í™•ë¥  ë°€ë„ í•¨ìˆ˜ $\pi(z)$ë¥¼ ì•Œê³  ìˆë‹¤ê³  ê°€ì •í•˜ê³ , **invertibleí•œ** ì¼ëŒ€ì¼ ëŒ€ì‘ í•¨ìˆ˜ $f$ë¥¼ ì‚¬ìš©í•´ì„œ, $x = f(z)$ë¡œ í‘œí˜„ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ê·¸ë¦¬ê³  **í•¨ìˆ˜ $f$ëŠ” invertible**í•˜ë‹¤ê³  ê°€ì •í–ˆê¸° ë•Œë¬¸ì—, $z=f^{-1}{(x)}$ë¡œ í‘œí˜„ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ì—¬ê¸°ì„œ ì €í¬ê°€ í•˜ê³  ì‹¶ì€ ê²ƒì€ $x$ì˜ unknown í™•ë¥  ë¶„í¬ì¸  $p(x)$ë¥¼ êµ¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

í™•ë¥  ë¶„í¬ì˜ ì •ì˜ë¥¼ í†µí•´,  $\int p(x)dx = \int \pi(z)dz = 1$ ë¡œ í‘œí˜„ì´ ê°€ëŠ¥í•˜ê³  

**Change of Variable**ì„ ì ìš©í•˜ë©´, $\int p(x)dx = \int \pi(f^{-1}(x))d f^{-1}(x)$ ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

ì´ê²ƒì„ $p(x) = \pi (z)\Bigl|\frac{dz}{dx}\Bigl| = \pi(f^{-1}(x))\Bigl|\frac{df^{-1}}{dx}\Bigl| = \pi(f^{-1}(x))\Bigl|(f^{-1})'(x)\Bigl|$ ì™€ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆê³  êµ¬í•˜ê³  ì‹¶ì—ˆë˜ $**p(x)$ë¥¼ $z$ì˜ í™•ë¥ ë°€ë„í•¨ìˆ˜ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤**.

ì‹¤ì œë¡œëŠ” ê³ ì°¨ì›ì˜ ë³€ìˆ˜ë“¤ì„ ë‹¤ë£¨ê¸° ë•Œë¬¸ì—, ì´ë¥¼ ë‹¤ë³€ìˆ˜(Multi-variable)ê´€ì ìœ¼ë¡œ ë‹¤ì‹œ í‘œí˜„ì„ í•˜ê¸° ìœ„í•´ í–‰ë ¬ì„ ì‚¬ìš©í•˜ë©´,

$\mathbf{z} \sim \pi(\mathbf{z}), \mathbf{x} = f(\mathbf{z}), \mathbf{z} = f^{-1}(\mathbf{x})\\
p(\mathbf{x})=\pi(\mathbf{z})\Bigl|\text{det}\frac{d\mathbf{z}}{d\mathbf{x}}\Bigl| = \pi(f^{-1}(\mathbf{x}))\Bigl|\text{det}\frac{df^{-1}}{d\mathbf{x}}\Bigl|$

ì™€ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## Jacobian Matrix and Determinant

![Untitled](assets/img/2023-11-16-write-Variational Inference_with_Normalizing_Flows/Untitled%209.png)

**[Jacobian matrix](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant)**ëŠ” ìœ„ì™€ê°™ì´ ë²¡í„° $\text{x}$, $\text{y}$ì— ëŒ€í•œ ì¼ì°¨ í¸ë¯¸ë¶„ì„ í–‰ë ¬ë¡œ ë‚˜íƒ€ë‚¸ ê²ƒì…ë‹ˆë‹¤.

ì¦‰, ìš°ë¦¬ê°€ $n$ì°¨ì› ì…ë ¥ ë²¡í„° $\text{x}$ë¥¼ $m$ì°¨ì› ì¶œë ¥ ë²¡í„° $\text{y}$ë¡œ mappingí•˜ëŠ” ($\text{y}:\mathbb{R}^n \mapsto \mathbb{R}^m$)í•¨ìˆ˜ê°€ ì£¼ì–´ì§€ë©´ ì´ í•¨ìˆ˜ì˜ ëª¨ë“  1ì°¨ í¸ë¯¸ë¶„ í•¨ìˆ˜ í–‰ë ¬ì„ ì´ë ‡ê²Œ Jacobian matrixë¡œ ê°„ë‹¨í•˜ê²Œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![Untitled](assets/img/2023-11-16-write-Variational Inference_with_Normalizing_Flows/Untitled%2010.png)

![Untitled](assets/img/2023-11-16-write-Variational Inference_with_Normalizing_Flows/Untitled%2011.png)

**[Determinant](https://en.wikipedia.org/wiki/Determinant)**ëŠ” í–‰ë ¬ì„ ëŒ€í‘œí•˜ëŠ” ê°’ìœ¼ë¡œ, ì •ë°©í–‰ë ¬(Square Matrix)ì— ì–´ë–¤ íŠ¹ì •í•œ ë°©ë²•ìœ¼ë¡œ í•˜ë‚˜ì˜ ìˆ˜ë¥¼ ëŒ€ì‘ì‹œí‚¤ëŠ” ì¼ì¢…ì˜ í•¨ìˆ˜ì…ë‹ˆë‹¤.

Determinantì˜ ì„±ì§ˆì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

(1) í–‰ë ¬Â $A$ì˜ ì„ì˜ì˜ í–‰ì— ìŠ¤ì¹¼ë¼ ê³±ì„ í•œ ë’¤ ë‹¤ë¥¸ í–‰ì— ë”í•´Â $B$ë¥¼ ë§Œë“¤ì—ˆì„ ë•Œ ë‘ í–‰ë ¬ì˜ determinantëŠ” ê°™ë‹¤.

(2) í–‰ë ¬Â $A$ì˜ ì„ì˜ì˜ í–‰ì„ ë‹¤ë¥¸ í–‰ê³¼ ë°”ê¾¸ì–´Â $B$ë¥¼ ë§Œë“¤ì—ˆì„ ë•ŒÂ ğ‘‘ğ‘’ğ‘¡$B$$=âˆ’$ğ‘‘ğ‘’ğ‘¡$A$

(3) í–‰ë ¬Â $A$ì˜ ì„ì˜ì˜ í–‰ì— ìŠ¤ì¹¼ë¼ ê³±ì„ í•´Â $B$ë¥¼ ë§Œë“¤ì—ˆì„ ë•ŒÂ ğ‘‘ğ‘’ğ‘¡$B$$=$$k$ğ‘‘ğ‘’ğ‘¡$A$

(4)Â **ì‚¼ê°í–‰ë ¬(triangular matrix)**ì˜ í–‰ë ¬ì‹ì€ ì£¼ ëŒ€ê°ì›ì†Œë“¤ì˜ ê³±ê³¼ ê°™ë‹¤.

(5) í–‰ë ¬Â $A$ê°€Â **ê°€ì—­(invertible)**ì„ê³¼Â ğ‘‘ğ‘’ğ‘¡$A$$â‰ 0$.

(6)Â ğ‘‘ğ‘’ğ‘¡$ğ´^T$$=$ğ‘‘ğ‘’ğ‘¡$A$

(7)Â ğ‘‘ğ‘’ğ‘¡$AB$$=$(ğ‘‘ğ‘’ğ‘¡$A$)(ğ‘‘ğ‘’ğ‘¡$B$)

ì´ì œ **Normalizing Flow**ì˜ ë™ì‘ ê³¼ì •ì€ ê°„ë‹¨í•˜ê²Œ í‘œí˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

![Untitled](assets/img/2023-11-16-write-Variational Inference_with_Normalizing_Flows/Untitled%2012.png)

($x$ëŠ” high dimensional data, $z$ëŠ” latent variable)

![Untitled](assets/img/2023-11-16-write-Variational Inference_with_Normalizing_Flows/Untitled%2013.png)

ì—¬ê¸°ì„œ $**z$ì˜ í™•ë¥  ë¶„í¬ë¥¼ ì•Œê³  ìˆë‹¤ë©´ $x$ì˜ í™•ë¥  ë¶„í¬ë¥¼ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤**.

ì•„ê¹Œ Change of Variable Theorem ì„¤ëª…ì—ì„œì™€ ë§ˆì°¬ê°€ì§€ë¡œ, $x = f(z)$, $z=f^{-1}{(x)}$ì¸ ìƒí™©ì—ì„œ, 

$p(x)$ $=$ $p(z)detJ$ë¡œ $z$ì˜ í™•ë¥  ë¶„í¬ì— scalarê°’ì¸ determinantë¥¼ ê³±í•´ì„œ $x$ì˜ í™•ë¥  ë¶„í¬ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê·¸ë¦¬ê³  ì–‘ë³€ì— $log$ë¥¼ ì”Œìš°ë©´,  $log($$p(x))$ $=$ $log(p(z))+log(detJ)$ë¡œ í‘œí˜„ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ê·¸ëŸ°ë° ì‹¤ì œ ë°ì´í„°ì¸  $x$ëŠ” ë³´í†µ ë§¤ìš° ë³µì¡í•œ ë¶„í¬ë¥¼ ê°€ì§€ê¸° ë•Œë¬¸ì— $x$ì™€ $z$ë¥¼ í•˜ë‚˜ì˜ í•¨ìˆ˜ë¡œ ë°”ë¡œ ì—°ê²°í•˜ê¸°ëŠ” ì–´ë µìŠµë‹ˆë‹¤.

![Untitled](assets/img/2023-11-16-write-Variational Inference_with_Normalizing_Flows/Untitled%2014.png)

$p(x)=p(z_{1})detJ_{1}$ 

$p(z_{1})=p(z_{2})detJ_{2}$ 

$p(z_{2})=p(z_{3})detJ_{3}$ 

â€¦.

$p(z_{n-1})=p(z_{n})detJ_{n}$ 

ê·¸ë˜ì„œ ì´ë ‡ê²Œ ë§ì€ í•¨ìˆ˜ë¥¼ í†µí•´ì„œ mappingì„ í•´ì£¼ëŠ” ê²ƒì´ê³  $p(x) = p(z_{n})\Pi_{n}(detJ_{n})$ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ìµœì¢…ì ìœ¼ë¡œ log likelihoodëŠ”  $log($$p(x))$ $=$ $log(p(z_{n}))+\Sigma_n log(detJ_{n})$ë¡œ í‘œí˜„ë©ë‹ˆë‹¤.

![Untitled](assets/img/2023-11-16-write-Variational Inference_with_Normalizing_Flows/Untitled%2015.png)

ë”¥ëŸ¬ë‹ì—ì„œ Normalizing Flowë¥¼ ì ìš©í•˜ì—¬ $x$ì˜ í™•ë¥  ë¶„í¬ë¥¼ ì•Œê¸° ìœ„í•´ì„œëŠ” **2ê°€ì§€ ì¡°ê±´ì´ ê¼­ ì¶©ì¡±**ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

1. **í•¨ìˆ˜ $f$ì˜ ì—­í•¨ìˆ˜ê°€ ì¡´ì¬í•´ì•¼ í•¨** (**invertible** $f$)
2. **Jacobianì˜ Determinantë„ ê³„ì‚° ê°€ëŠ¥í•´ì•¼ í•¨**

ì´ 2ê°€ì§€ ì¡°ê±´ì„ ê³ ë ¤í•˜ì—¬ í•¨ìˆ˜  $f$ë¥¼ ì„ íƒí•´ì•¼ í•˜ê³ , ì´ flow í•¨ìˆ˜ë“¤ì„ ëª¨ë¸ë§ˆë‹¤ ì–´ë–»ê²Œ êµ¬í˜„í•˜ëŠ” ì§€ê°€ ë‹¤ë¦…ë‹ˆë‹¤.

![Untitled](assets/img/2023-11-16-write-Variational Inference_with_Normalizing_Flows/Untitled%2016.png)

![Untitled](assets/img/2023-11-16-write-Variational Inference_with_Normalizing_Flows/Untitled%2017.png)

## **Finite Flows and Infinitesimal Flows**

Finite Flowsì™€ Infinitesimal Flowsì€ Normalizing Flowsì—ì„œ ì‚¬ìš©ë˜ëŠ” ë‘ ê°€ì§€ ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ì…ë‹ˆë‹¤.

# Experiments

![Effect of normalizing flow on two distributions](assets/img/2023-11-16-write-Variational Inference_with_Normalizing_Flows/Untitled%2018.png)

Effect of normalizing flow on two distributions

ì´ˆê¸° Unit Gaussian ë¶„í¬ë¡œë¶€í„° ë³µì¡í•œ ë¶„í¬ ë³€í™˜ ê°€ëŠ¥í•œ normalizing flowì˜ performanceë¥¼ ë³´ì—¬ì£¼ê³  ìˆìŒ

![Approximating four non-Gaussian 2D distributions. The images represent densities for each energy function in table 1 in the range (âˆ’4,4)$^2$. (a) True posterior; (b) Approx posterior using the normalizing flow; (c) Approx posterior using NICE; (d) Summary results comparing KL-divergences between the true and approximated densities for the first 3 cases](assets/img/2023-11-16-write-Variational Inference_with_Normalizing_Flows/Untitled%2019.png)

Approximating four non-Gaussian 2D distributions. The images represent densities for each energy function in table 1 in the range (âˆ’4,4)$^2$. (a) True posterior; (b) Approx posterior using the normalizing flow; (c) Approx posterior using NICE; (d) Summary results comparing KL-divergences between the true and approximated densities for the first 3 cases

4ê°œì˜ Gaussianì´ ì•„ë‹Œ ë¶„í¬ì˜ ê·¼ì‚¬ ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

- (a) **True Posterior**: ê°ê°ì˜ ê²½ìš°ì— ëŒ€í•œ ì‹¤ì œ ì‚¬í›„ ë¶„í¬ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
- (b) **Approximate Posterior using Normalizing Flow**: Normalizing Flowë¥¼ ì‚¬ìš©í•˜ì—¬ ê° posterior distributionì„ ê·¼ì‚¬í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
- (c) **Approximate Posterior using NICE**: NICE ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ê° posterior distributionì„ ê·¼ì‚¬í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
- (d) **Comparison of KL-divergences**: **True Posterior**ì™€ **Approximate Posterior ê°„**ì˜ Kullbackâ€“Leibler divergenceì„ ë¹„êµí•©ë‹ˆë‹¤. ì‹¤ì œ ë¶„í¬ì™€ ê·¼ì‚¬ ë¶„í¬ ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.

![Untitled](assets/img/2023-11-16-write-Variational Inference_with_Normalizing_Flows/Untitled%2020.png)

![Untitled](assets/img/2023-11-16-write-Variational Inference_with_Normalizing_Flows/Untitled%2021.png)