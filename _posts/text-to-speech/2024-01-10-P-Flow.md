---
layout: post
title: "[Text-to-Speech] P-Flow"
description: >
  P-Flow 논문 요약
category: seminar
tags: text-to-speech
author: jh_yun
comments: true
---

# P-Flow

- S. Kim., et al, “P-Flow: A Fast and Data-Efficient Zero-Shot TTS through Speech Prompting”, 2023

# Goal

- Speech-prompted text encoder와 Flow-matching decoder를 활용한 Zero-shot TTS 모델을 제안

# Motivation

- 최근 연구의 단점
    - 최근의 Zero-shot TTS 연구들은 복잡한 학습 세팅, 추가적인 양자화, 사전학습, 대용량 데이터 셋 등을 필요로 하여 모델이 무겁고 많은 비용을 소모한다는 단점이 있습니다.
    - 최근에 대규모 neural codec **언어모델** 을 활용한 Zero-shot TTS가 우수한 성능을 보였지만, 속도도 느리고 robustness도 부족하고 pre-trained neural codec respresentation에 의존합니다.
- 최근 연구들에서 언어모델의 In-context 학습 능력을 사용하여 모델의 성능을 높이는 성공사례가 있습니다.
- Flow-matching [Lipman, 2022] 모델은 생성모델 중에서도 속도가 빠르다는 것이 장점입니다.

# Contribution

- Non-AR Zero-shot TTS 모델에 **Speech-Prompt approach**를 제안하여 Speaker embedding approach를 뛰어넘고, Speaker adpatation을 위한 In-context learning 능력을 주었음.
- 최근 연구와 비교했을 때, 적은 데이터 셋과 작은 모델 크기로 우수한 제로샷 TTS 성능을 보여주었습니다.

---

# Method

## 1. Overview

- 저자들은 TTS 모델에게 zero-shot adapation을 할 수 있는 In-context 학습 능력을 주고자 했습니다.
- reference 발화에서 발화자의 정보를 추출하기 위해, reference 발화를 prompt로 바로 사용하는 speech prompting 방식을 채택했습니다.
- 효율적이면서 높은 성능의 samping을 위해 Flow-matching decoder를 사용합니다.

![Untitled](https://github.com/speech-team-korea/speech-team-korea.github.io/assets/144989499/bc78b679-bd1c-44d9-9997-dad67ef58768)

- Notation
    - $c$ : text
        - ex) “I love you”
    - $x$ : speech
        - ex) 화자 A의 “I love you”
    - $m^p$ : indicator mask
    - $x^p = ( 1-m^ p)\cdot x$ : speech segment of 3 sec
        - ex) 화자 A의 “love” (3초짜리 segment)
- P-Flow 모델은 text $c$와 발화 segment  $x^p$ 를 입력으로 받아, 발화 $x$를 복원하는 목적으로 학습이 이루어집니다.

$$
p(x | c, x^p)
$$

## 2. Speech-prompted text encoder

![Untitled](https://github.com/speech-team-korea/speech-team-korea.github.io/assets/144989499/68878eca-65b4-42c5-8e20-4a145feb3558)

$$
h_c = f_{enc}(x ^ p ,   c )
$$

Speech-prompted text encoder는 인풋으로 text $c$와 발화 segment  $x^p$을 받아, speaker-conditional text representation $h_c$ 을 만드는 것이 목적입니다. 

대규모 codec 언어모델과 유사하게, 트랜스포머 아키텍쳐를 적용하였습니다. 

Monotonic alignment search (MAS) 알고리즘을 사용해서 duration에 관한 정보를 얻고, $h_c$ 를 upsampling 하여 $h$를 Aligned representation을 만듭니다. 

Speech-prompted text encoder가 reference 발화(speech prompt)에서 발화자 정보를 잘 추출할 수 있도록, text encoder representation $h$ 와 Mel-spectrogram 사이의 거리를 줄이는 encoder loss를 사용했습니다. 

이 방법론은 Grad-TTS에서 제안하였던 것으로, 이렇게 하게 되면 발화자의 특성을 잘 포착하고 재현할 수 있습니다.

특히 저자들은 loss를 계산하는 과정에서 다음과 같은 사실을 알아냈습니다. text encoder는 $x$ 내에서의 $x^p$의 위치 정보를 받지 않았지만, 모델이 trivial solution을 좇아 $x^p$를 copy-pasting 하면서 학습이 붕괴되는 현상이 발생한다는 것 입니다. 따라서, loss를 계산할 때 발화 segment 부분 $x^p$에 해당하는 부분만 제외하여 학습이 붕괴되는 것을 방지했습니다.

$$
L_{enc} = MSE(h,x), \quad L_{enc}^{p}  = MSE(h \cdot m^p, x\cdot m^{p} )
$$

## 3. Flow matching decoder

![Untitled](https://github.com/speech-team-korea/speech-team-korea.github.io/assets/144989499/755b5aeb-c6e5-407d-8cc3-077dae5f8e49)

아래와 같은 확률분포를 모델링하기 위해 Flow matching decoder를 사용합니다.

$$
p(x |  c, x^p) = p(x|h)
$$

아래 수식의 흐름에서 편의성을 위해 $h$ 는 제외하였습니다.

단순한 분포 $p_{0} (x) = N(x; 0,I)$에서 데이터 분포 $p_{1}(x)\approx q(x)$ 사이의 Mapping으로 사용되는 Flow  $\phi_{t}: [0,1] \times \mathbb{R}^d \to \mathbb{R}^d$  는 아래의 ODE로 정의할 수 있습니다.

$$
\frac{d}{dt} \phi_t 
(x)= v_t (\phi_t(x)); \quad \phi_0(x)=x
$$

여기서 $v_ {t} (x)$ 는 시간에 따른 probability flow의 경로를 정하는 vector field 입니다. 그리고 학습이 가능하기 때문에 다음과 같이 표현하겠습니다 $v_{t} ( \phi (x); \theta)$. 

$p_1$에서 샘플링하는 방법은 먼저, $p_0$에서 샘플링한 후에, 위의 ODE를 해결하면 됩니다. 

[Lipman, 2022]에서 marginal flow $\phi_ {t} (x)$를 결정하는 것은 실제로 불가능하다는 것을 밝혔고, [Lipman, 2022]에서는 아래와 같이 ($x_1$이 조건으로 주어진) conditional flow $\phi _{t, x_ {1}} (x)$ 여러개를 marginalizing 함으로써 marginal flow $\phi_ {t} (x)$를 구하는 방법을 제안했습니다.

$$
\phi _{t, x_1} (x) = \sigma_ {t} (x_{1} ) x + \mu _{t} (x_ {1} )
$$

여기서 $\sigma _ {t} (x _ {1})$ 과 $\mu _ {t} (x _ {1})$은 두 개의 가우시안 분포 $p_1$과 $p_0$ 사이의 변환의 매개변수화를 위한 time-conditional affine 변환입니다. 여기서 $p_1$은 $q$에 대한 가우시안 혼합 approximation으로 정의합니다.

[Lipman, 2022]에서는 SDE처럼 복잡하게 trajectory를 정의하지 않고, 아래와 같이 간단한 선형 trajectory를 정의했습니다.

$$
\mu_ {t} (x) = t x _{1}, \quad \sigma_{t} (x) = 1- (1-\sigma_{\text{min}}) t
$$

Vector field는 아래의 conditional flow matching objective를 통해서 학습됩니다.

$\frac{d}{dt} \phi_{t,x_1} (x_0) = v_t (\phi_{t, x_1}(x_0))$ by above equation

$$
L _ {\text{CFM}} ( \theta ) = \mathbb{E}_{t \sim U[0,1],x_{1} \sim q(x_{1}),x_0\sim p_{0} (x_{0})} || v_t(\phi_{t,x_1} (x_0);\theta ) - \frac{d}{dt} \phi_{t,x_1} (x_0)|| ^2
$$

위의 선형 trajectory로 정의한 $\mu_ {t} (x)$와 $\sigma _ {t} (x)$ 를 위의 $L_ {CFM}$ 에 대입하면 Vector field $v_t(\cdot ; \theta)$ 가 닮으려는 objective를 아래와 같이 쓸 수 있습니다.

$$
L _ {\text{CFM}} ( \theta ) = \mathbb{E}_{t \sim U[0,1],x_{1} \sim q(x_{1}),x_0\sim p_{0} (x_{0})} || v_t(\phi_{t,x_1} (x_0);\theta ) -  (x_1- (1-\sigma_{\text{min}}) x_0 || ^2
$$

Decoder loss를 계산할때도 마찬가지로, trivial solution을 피하기 위해  $x^ p$ 에 대응하는 loss를 마스킹해주었습니다. 최종적으로 $v_ {t} (x_ {t} ; \theta)$ 를 $\hat{v} _ {\theta} ( x_ {t}, h,t)$ 로 표현할 수 있고, 최종적으로 아래의 Masked flow matching Objective 로 학습이 진행됩니다. 

$$
L ^ p _ {\text{CFM}} ( \theta ) = \mathbb{E}_{t ,x_{1} \sim q(x_{1}),x_0\sim p_{0} (x_{0})} || m^p \cdot (\hat{v}_\theta(\phi_{t,x_1} (x_0),h,t ) -  (x_1- (1-\sigma_{\text{min}}) x_0 ) || ^2
$$

### Sampling

1차 오일러 메소드, 10 step

$$
x_0 \sim \mathcal{N} (0, I); \quad x_{t + \frac{1}{N}} = x_t + \frac{1}{N} \hat{v}_{\theta} (x_{t},h,t)
$$

### Guided sampling

저자들은 classifier-free guidance 방법을 통해 발음 정확도를 높일 수 있다는 것을 발견했습니다.

$$
x_{t + \frac{1}{N}} = x_t + \frac{1}{N} (

\hat{v}_{\theta} (x_{t},h,t)

+ \gamma (\hat{v}_{\theta} (x_{t},h,t) - \hat{v}_{\theta} (x_{t},\bar{h},t)) )

$$

여기서  $\bar{h}$는 $h$를 시간축에 따라 평균을 계산하고, 이를 다시 시간축으로 Upsampling 하여 구합니다.

# Experiments

- Ablation study for prompting based approach

![Untitled](https://github.com/speech-team-korea/speech-team-korea.github.io/assets/144989499/e534b344-3134-4bec-a9d5-4d250b6e8453)

- Objective evaluation

![Untitled](https://github.com/speech-team-korea/speech-team-korea.github.io/assets/144989499/b70e3bbd-e6fa-462a-8775-6dab33a00248)

- Subjective evalutation

![Untitled](https://github.com/speech-team-korea/speech-team-korea.github.io/assets/144989499/8e82d349-cb4e-4ff8-9d12-b701a364fde0)

- Experiment for guidance scale $\gamma$  and Euler steps $N$

![Untitled](https://github.com/speech-team-korea/speech-team-korea.github.io/assets/144989499/62331ae4-ecea-4d26-a242-b6d00459931d)

![Untitled](https://github.com/speech-team-korea/speech-team-korea.github.io/assets/144989499/a6fa8cb8-e477-4251-a927-f2ba6dbaf393)

# Discussion

- 개인적인 생각
    - 스피치-프롬프트 텍스트 인코더에 학습 때 넣어주는 Mel-spectrogram과 인퍼런스 때 넣어주는 Mel-spectrogram의 시나리오가 다르지 않은가?
    - 텍스트와 스피치를 같은 모듈에 넣어주는 것이 실제로 효과적일까?
- 오픈리뷰에서 가져온 것
    - long text prompt에 대해서는 neural codec 모델을 사용하는 것이 더 좋아보이는데요?
    - 데모에서 timbre 제대로 안 잡히는 것 같은데요?
    - 왜 Flow-matching을 사용해야만 했나요? zero-shot 시나리오에서 Flow-matching이 더 좋은 이유가 있나요?