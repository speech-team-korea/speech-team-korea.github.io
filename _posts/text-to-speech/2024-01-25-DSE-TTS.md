---
layout: post
title: "[Text-to-Speech] DSE-TTS"
description: >
  DSE-TTS 논문 요약
category: seminar
tags: text-to-speech
author: jh_yun
comments: true
---


# DSE-TTS

<blockquote style="border-left: 2px solid; padding-left: 10px; margin-left: 0;">
Sen Liu, Yiwei Guo, Chenpeng Du, Xie Chen, Kai Yu <br>
"DSE-TTS: Dual Speaker Embedding for Cross-Lingual Text-to-Speech"<br>
Accepted by INTERSPEECH 2023 <br>
[<a href="https://arxiv.org/pdf/2306.14145.pdf">Paper</a>] [<a href="https://goarsenal.github.io/DSE-TTS">Demo</a>] <br>
</blockquote>


# Goal
- Dual 스피커 임베딩 방식과 Vector-quantized 어쿠스틱 피쳐를 활용한 Cross-lingual TTS 모델 제안

# Motivation

- 이전의 Cross-lingual TTS 연구들은 어쿠스틱 피쳐로 Mel-spectrogram을 사용
    - Mel-spectrogram의 특징
        - 시간축과 주파수축이 높은 상관관계성을 가지며 많은 양의 스피커 정보를 담고 있음
        - 상관관계가 있는 요소들간의 분리가 어렵다는 문제가 있음
    - 이전의 Cross-lingual TTS 연구
        - [[SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech](https://arxiv.org/pdf/2206.12132.pdf), 2022]
        - [[Cross-lingual text-to-speech synthesis via domain adaptation and perceptual similarity regression in speaker space](https://www.isca-archive.org/interspeech_2020/xin20_interspeech.pdf), 2020]
- Speech-based Self-supervised learning (SSL) 모델을 활용한 Vector-quantized 어쿠스틱 피쳐의 특징
    - Vector-quantized 어쿠스틱 피쳐를 활용한 TTS 모델들의 등장 [[VQ-TTS](https://arxiv.org/pdf/2204.00768.pdf), 2022]
    - Vector-quantized 어쿠스틱 피쳐는 스피커에 대한 정보를 상대적으로 덜 담고 있음
        
        ![Untitled](https://github.com/speech-team-korea/speech-team-korea.github.io/assets/144989499/63cc7797-ac6a-432d-b4f4-fb890c934f52)
        

# Contribution

- Dual 스피커 임베딩 방식을 제안하여 합성된 발화의 자연스러움과 화자 유사도를 향상시킴
- Vector-quantized 어쿠스틱 피쳐가 스피커 정보를 적게 담고 있다는 것을 알아내고 이를 활용함

---

# Dual Speaker Embedding TTS

![Untitled](https://github.com/speech-team-korea/speech-team-korea.github.io/assets/144989499/d2c851b8-39f3-4162-8313-bb2d855da7b0)

### 시나리오 설정

- Intra-lingual (Input text와 Target speaker speech의 Language가 동일)
    - Input text
        - 한국어 “안녕하세요 만나서 반갑습니다.”
    - Target speaker $=$ Native speaker
        - 한국인 준학 曰 “오늘 정말 배고프네요.”
    - Synthesized speech
        - 한국인 준학 曰 “안녕하세요 만나서 반갑습니다.”

- Cross-lingual (Input text와 Target speaker speech의 Language가 상이)
    - Input text
        - 영어 “What do you want to do?”
    - Target speaker $\ne$ Native speaker
        - 한국인 준학 曰 “오늘 정말 배고프네요.”
    - Synthesized speech
        - 한국인 준학 曰 “What do you want to do?”

## Vector-quantized based TTS

기존 Cross-lingual TTS에서 해결해야하는 문제점은 Target 발화자의 Timbre은 유지하면서 Accent는 제거해야하는데, **발화자와 언어의 Disentanglement가 어렵다**는 점입니다. 이러한 문제점은 **어쿠스틱 피쳐로 Mel-spectrogram을 사용**할 때 더욱 부각됩니다. 

하지만 **Vector-Quantized (VQ) 어쿠스틱 피쳐**는 상대적으로 **스피커 정보를 적게 가지며**, VQ 기반의 TTS 모델은 위의 문제점이 어느정도 해결됩니다.

Backbone 모델로 [VQ-TTS, 2022]를 사용하여 모델은 크게 2개의 구성요소로 나뉩니다.

- txt2vec : 텍스트 정보로부터 VQ 어쿠스틱 피쳐를 만드는 **어쿠스틱 모델**
    - 다양한 언어의 Textual하고 **Lingustic한 특징**들을 모델링하는 데에 집중
- vec2wav : 어쿠스틱 피쳐로부터 Waveform을 만드는 **보코더 모델**
    - **Target 스피커의 Timbre**와 **Target 언어**의 자연스러운 결합을 모델링하는 데에 집중

[[wav2vec 2.0](https://arxiv.org/pdf/2006.11477.pdf), 2020] 모델을 사용하여 2개의 codebook에 각 320개의 code words가 존재, 해당 모델은 10,000 시간 Mandarin data로 사전학습이 되었습니다. (Mandarin data만 사용한 이유는 알리지 않음)

## Dual speaker embedding

해당 모델은 txt2vec (어쿠스틱 모델)과 vec2wav (보코더 모델)에게 각각 1개의 스피커 임베딩을 입력으로 주어, **총 2개의 스피커 임베딩을 사용**합니다. 예시는 아래와 같습니다.

### 학습과정 시나리오 예시

[Intra-lingual TTS]

- Target speaker(Native speaker) : 한국인 준학 曰 “정말 값진 하루네요.”
- Input text : 한국어 “XX  XXX  XX  XXX” (미확정)
- txt2vec 에 사용되는 스피커 임베딩 : **준학에 대한 임베딩**
- vec2wav 에 사용되는 스피커 임베딩 : **준학에 대한 임베딩**

### 인퍼런스 과정 시나리오 예시

[Intra-lingual TTS]

- Target speaker(Native speaker) : 한국인 준학 曰 “정말 값진 하루네요.”
- Input text : 한국어 “OOOO  O  OOO  OO” (원하는 텍스트)
- txt2vec 에 사용되는 스피커 임베딩 : 임의의 **한국인에 대한 임베딩 (Native speaker)**
- vec2wav 에 사용되는 스피커 임베딩 : **준학에 대한 임베딩 (X-vector of target speaker)**

[Cross-lingual TTS]

- Target speaker (Non-native speaker) : 미국인 브래드피트 曰 “Keep dreaming, keep believing.”
- Input text : 한국어 “OOOO  O  OOO  OO” (원하는 텍스트)
- txt2vec 에 사용되는 스피커 임베딩 : 임의의 **한국인에 대한 임베딩 (Native speaker)**
- vec2wav 에 사용되는 스피커 임베딩 : **브래드피트에 대한 임베딩 (X-vector of target speaker)**

즉, 실제로 모델이 학습이 끝난 후에 Inference 시에는 Intra-lingual 혹은 Cross-lingual 에 상관없이 txt2vec (어쿠스틱 모델) 은 Natives speaker의 스피커 임베딩을 사용합니다. 

반대로, vec2wav (보코더 모델) 은 원하는 Target speaker 의 스피커 임베딩 [[x-vector](https://www.danielpovey.com/files/2018_icassp_xvectors.pdf), 2018] 을 사용합니다.

이렇게 설정할 경우, 어쿠스틱 모델은 입력으로 들어온 텍스트와 Native speaker의 Linguistic speaking 스타일을 결합한 VQ 어쿠스틱 피쳐를 만들 수 있습니다. 즉, 발화의 자연스러움을 향상시킬 수 있습니다.

또한, 보코더 모델은 Target speaker 의 Timbre를 집중적으로 모델링하여 화자 유사도를 향상시킬 수 있습니다.

추가적으로 Cross-lingual 시나리오에서는 Target speaker의 목소리와 유사한 Timbre를 얻기 위해, txt2vec이 예측한 Native speaker의 pitch distribution을 아래와 같이 변환하여 Target speaker의 pitch와 매칭시켜주었습니다.

$$
P_{\text{tgt}}= \sigma_{\text{tgt}} \frac{P_{\text{native}}-\mu_{\text{native}}}{\sigma_{\text{native}}} + \mu_{\text{tgt}}
$$

여기서의 $\mu$와 $\sigma$는 전부 Training set에서 추출하였습니다. (보완해야할 점으로 추정됨)

# Experiments

## Dataset

- Mandarin (ZH) ← [Aishell3](https://www.openslr.org/93/) 사용
- English (EN) ← [LibriTTS](https://openslr.org/60/) 사용
- Spanish (ES) ← [M_AILABS](https://www.caito.de/2019/01/03/the-m-ailabs-speech-dataset) 사용
- German (DE) ← [M_AILABS](https://www.caito.de/2019/01/03/the-m-ailabs-speech-dataset) 사용

![Untitled](https://github.com/speech-team-korea/speech-team-korea.github.io/assets/144989499/af3e8623-57d7-45fb-b846-b6daec15219b)

## Result

- Cross-lingual

![Untitled](https://github.com/speech-team-korea/speech-team-korea.github.io/assets/144989499/6531bf30-621a-4cd4-9a54-36a3e5b6d13b)

- Intra-lingual

![Untitled](https://github.com/speech-team-korea/speech-team-korea.github.io/assets/144989499/760142b9-7f15-4caf-b111-79840d10ab60)

---

# Reference

-
