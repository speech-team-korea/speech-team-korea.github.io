---
layout: post
title: "Glow: Generative Flow with Invertible 1x1 Convolutions"
description: >
  ì•„ì§ ë¯¸ì™„ì„±ì…ë‹ˆë‹¤..!
category: seminar
tags: generative-model
author: jh_cha
comments: true
---

# Glow: Generative Flow with Invertible 1Ã—1 Convolutions

# Goal

### Proposing Glow, a simple type of generative flow using an invertible 1 x 1 convolution

# Motivations

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë¨¼ì € machine learningì—ì„œì˜ 2ê°€ì§€ major problemë“¤ì— ëŒ€í•´ì„œ ì–¸ê¸‰í•˜ê³  ìˆìŠµë‹ˆë‹¤.

### Data-efficiency

- ì¸ê°„ì²˜ëŸ¼ few datapointsë§Œìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ëŠ¥ë ¥ì´ ë¶€ì¡±í•¨

### Generalization

- Taskë‚˜ Contextì˜ ë³€í™”ì— robustnessë¥¼ ê°€ì§€ì§€ ëª»í•¨

ì´ëŸ¬í•œ limitationë“¤ì„ ê¸°ì¡´ì˜ generative modelë“¤ì€ ì‚¬ëŒì˜ supervisionì´ë‚˜ labelingì—†ì´ input ë°ì´í„°ì˜ ì˜ë¯¸ìˆëŠ” featureë“¤ì„ í•™ìŠµí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ê°œì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ë³´í†µ Generative modelì€ ì‹¤ì œ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ëª¨ë¸ë§í•˜ê³ , ëª¨ë¸ë§í•œ ë¶„í¬ì—ì„œ samplingì„ í†µí•´ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±(Generate)í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## Types of Generative Models

### GANs [Goodfellow et al., 2014]
![ì¶œì²˜ : [https://lilianweng.github.io/posts/2018-10-13-flow-models/](https://lilianweng.github.io/posts/2018-10-13-flow-models/)](assets/img/2023-11-16-write-Glow/fig.png)

ì¶œì²˜ : [https://lilianweng.github.io/posts/2018-10-13-flow-models/](https://lilianweng.github.io/posts/2018-10-13-flow-models/)

**ì¥ì **

- Synthesize large and realistic images

**ë‹¨ì **

- No encoder to infer the latents
- Datapoints can not be directly represented in a latent space
- Difficulty of optimization and assessing overfitting and generalization

GANì€ ì•”ì‹œì (implicitly)ìœ¼ë¡œ ì‹¤ì œ í•™ìŠµ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.

### VAEs [Kingma and Welling, 2013, 2018]

![ì¶œì²˜ : [https://lilianweng.github.io/posts/2018-10-13-flow-models/](https://lilianweng.github.io/posts/2018-10-13-flow-models/)](assets/img/2023-11-16-write-Glow/fig1.png)

ì¶œì²˜ : [https://lilianweng.github.io/posts/2018-10-13-flow-models/](https://lilianweng.github.io/posts/2018-10-13-flow-models/)

**ì¥ì **

- Parallelizability in training and synthesis

**ë‹¨ì **

- Challenging to optimize
- Infer only approximately the value of the latent variables

**VAE(Various AutoEncoder)**ëŠ” **likelihood-based method**ì…ë‹ˆë‹¤.

ì—¬ê¸°ì„œ likelihoodëŠ” ë°ì´í„°ê°€ íŠ¹ì • ë¶„í¬ë¡œë¶€í„° ë§Œë“¤ì–´ì¡Œì„ í™•ë¥ ì„ ë§í•©ë‹ˆë‹¤. 

ì¦‰, likelihoodëŠ” ìƒì„± ëª¨ë¸ì´ ëª¨ë¸ë§í•˜ê³  ìˆëŠ” íŠ¹ì • í™•ë¥  ë¶„í¬ê°€ í•™ìŠµ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì„¤ëª…í•˜ê³  ìˆëŠ”ê°€ë¥¼ ë§í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì€ í•™ìŠµì„ í†µí•´ ì´ likelihoodë¥¼ ìµœëŒ€í™”í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ê°–ê²Œ ë˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.

VAEì—ì„œëŠ” ê³„ì‚°ì´ ì–´ë ¤ìš´(intractable) ì‚¬í›„í™•ë¥  ë¶„í¬(posterior distribution)ë¥¼ ë³´ë‹¤ ì‰¬ìš´ ë¶„í¬ë¡œ ê·¼ì‚¬í•˜ëŠ”ë°, Evidence Lower Bound(ELBO)ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ì˜ log-likelihoodë¥¼ ì•”ì‹œì ìœ¼ë¡œ(implicitly) í•™ìŠµí•©ë‹ˆë‹¤.

### Autoregressive models [Van den Oord et al, 2016]

![Visualization of a stack of *dilated* causal convolutional layers in WaveNet [Van den Oord et al, 2016]](assets/img/2023-11-16-write-Glow/fig2.png)

Visualization of a stack of *dilated* causal convolutional layers in WaveNet [Van den Oord et al, 2016]

**ì¥ì **

- Simplicity

**ë‹¨ì **

- Limited parallelizability in synthesis
- Troublesome for large images or video

**Autoregressive model**ë„ **likelihood-based method**ì…ë‹ˆë‹¤.

WaveNet ëª¨ë¸ì„ ì˜ˆì‹œë¡œ ì–˜ê¸°ë¥¼ í•˜ë©´, $T$ê°œì˜ ì˜¤ë””ì˜¤ ìƒ˜í”Œ $x$ë“¤ë¡œ ì´ë£¨ì–´ì§„ í•œ Waveform $\text{x}$ê°€ ìˆë‹¤ê³  í•  ë•Œ ($\text{x} = \{x_{1}, ... ,x_{T}\}$), Waveform $\text{x}$ì˜ joint probabilityë¥¼ ì•„ë˜ì™€ ê°™ì´ conditional probabilitiesì˜ ê³±ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$p(\text{x}
) = \Pi^T_{t=1}p(x_{t}|x_{1},...,x_{t-1})$ 

WaveNetì€ ë§¤ time step $t$ë§ˆë‹¤ ëª¨ë“  ì´ì „ time stepì—ì„œ ìƒì„±í•œ ìƒ˜í”Œë“¤ì„ ì¡°ê±´(conditioned)ìœ¼ë¡œ ìƒˆë¡œìš´ ì˜¤ë””ì˜¤ ìƒ˜í”Œ $x_{t}$ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. 

$x_{t}$ì˜ categorical distributionì„ outputìœ¼ë¡œ ë‚´ë³´ë‚´ê²Œ ë˜ëŠ” ê²ƒì´ê³  ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ì— ëŒ€í•˜ì—¬ ë°ì´í„°ì˜ log-likelihoodë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ìµœì í™”(optimization)ë¥¼ í•˜ê²Œ ë©ë‹ˆë‹¤. ì¦‰, ëª¨ë¸ì´ ì£¼ì–´ì§„ ë°ì´í„° ì…‹ì„ ìµœëŒ€í•œ ì˜ ì„¤ëª…í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ë„ë¡ í•™ìŠµì„ í•œë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.

ìœ„ì˜ ì˜ˆì‹œ ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, Synthesisë•Œ ì—°ì†ì ìœ¼ë¡œ sampleì„ ìƒì„±í•˜ê¸° ë•Œë¬¸ì— synthesisì˜ computational lengthëŠ” ë°ì´í„°ì˜ dimensionalityì— ë¹„ë¡€í•˜ê²Œ ë©ë‹ˆë‹¤.

ë”°ë¼ì„œ, Autoregressive modelì€ ì´ë¯¸ì§€ë‚˜ ë¹„ë””ì˜¤ì™€ ê°™ì´ ë°ì´í„°ì˜ dimensionalityê°€ í° ê²½ìš° ê³„ì‚° ë¹„ìš©ì´ ìƒë‹¹íˆ ì»¤ì§€ëŠ” ë¬¸ì œê°€ ìƒê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Flow-based generative models

![ì¶œì²˜ : [https://lilianweng.github.io/posts/2018-10-13-flow-models/](https://lilianweng.github.io/posts/2018-10-13-flow-models/)](assets/img/2023-11-16-write-Glow/fig3.png)

ì¶œì²˜ : [https://lilianweng.github.io/posts/2018-10-13-flow-models/](https://lilianweng.github.io/posts/2018-10-13-flow-models/)

**Flow-based model**ë„ **likelihood-based method**ì— ì†í•©ë‹ˆë‹¤.

Flow-based modelì€ GANì´ë‚˜ VAEì™€ ë‹¤ë¥´ê²Œ ëª…ì‹œì ìœ¼ë¡œ(explicitly) ë°ì´í„°ì˜ í™•ë¥  ë¶„í¬ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.

ì´ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ê³  ìˆëŠ” ë°©ë²•ì¸ Flow-based generative modelì€ **NICE**[Dinh et al., 2014]ì—ì„œ ì œì•ˆë˜ì—ˆê³ , **RealNVP**[Dinh et al., 2016]ì—ì„œ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤¬ìŠµë‹ˆë‹¤.

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Flow-basedì˜ ì¥ì ì„ ë‹¤ìŒê³¼ ê°™ì´ ì–¸ê¸‰í•˜ê³  ìˆìŠµë‹ˆë‹¤.

- **Exact latent-variable inference and log-likelihood evaluation**
    
    Flow-based ëª¨ë¸ì€ VAEì™€ ë‹¬ë¦¬, ë°ì´í„°ì˜ log-likelihoodë¥¼ ê·¼ì‚¬ì (approximately)ìœ¼ë¡œ ì¶”ì •í•˜ì§€ ì•Šê³  ì •í™•í•œ(exactly) ë¶„í¬ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    ê·¸ë¦¬ê³  VAEì—ì„œ Evidence Lower Bound(ELBO)ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ì˜ log-likelihoodë¥¼ ì•”ì‹œì ìœ¼ë¡œ(implicitly) ìµœì í™”(optimization)í•˜ì§€ë§Œ, Flow-based ëª¨ë¸ì€ ì •í™•í•˜ê²Œ ë°ì´í„°ì˜ log-likelihoodì˜ ìµœì í™”(optimization)ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    
- **Efficient to parallelize for both inference and synthesis**
    
    Flow-based ëª¨ë¸ì€ Autoregressive modelê³¼ ë‹¬ë¦¬, ë³‘ë ¬í™”(parallelization)ì„ í•˜ê¸°ì— íš¨ìœ¨ì ì¸ ëª¨ë¸ì…ë‹ˆë‹¤.
    
- **Useful latent space for downstream tasks**
    
    Flow-based ëª¨ë¸ì€ Autoregressive modelì´ë‚˜ GANê³¼ ë‹¬ë¦¬, datapointë“¤ì´ latent spaceìƒì—ì„œ í‘œí˜„(directly represented)ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    
    ê·¸ë˜ì„œ latent spaceìƒì—ì„œ ë°ì´í„° ê°„ì˜ interpolationì´ë‚˜ modificationì„ í†µí•´ ë‹¤ì–‘í•œ applicationì„ ê°€ëŠ¥í•˜ê²Œ í•´ì¤ë‹ˆë‹¤.
    
- **Significant potential for memory savings**
    
    Reversible neural networksì—ì„œ gradientë¥¼ ê³„ì‚°í•˜ëŠ” ë¹„ìš©ì€ depthì— ë”°ë¼ linearí•˜ê²Œ ì¦ê°€í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì¼ì •í•œ memory(that is constant)ë¥¼ ê°€ì§„ë‹¤ê³  í•©ë‹ˆë‹¤.
    
    [RevNet](https://proceedings.neurips.cc/paper_files/paper/2017/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.html) paper[Gomez et al., 2017]ì— ì˜ ì„¤ëª…ë˜ì–´ ìˆë‹¤ê³  í•˜ë‹ˆ í•„ìš”í•˜ë‹¤ë©´ ì°¸ê³ í•˜ë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤.
    

Flow-based Generative Modelì„ ì„¤ëª…í•˜ê¸°ì— ì•ì„œ, ì•Œê³  ìˆì–´ì•¼ í•  ê°œë…ì¸ the change of variable ruleê³¼ Jacobian determinantì— ëŒ€í•´ì„œ ë¨¼ì € ì†Œê°œí•˜ê² ìŠµë‹ˆë‹¤.

## Change of Variable Theorem

ì–´ë– í•œ ë³€ìˆ˜ ë˜ëŠ” ë‹¤ë³€ìˆ˜ë¡œ ë‚˜íƒ€ë‚¸ ì‹ì„ ë‹¤ë¥¸ ë³€ìˆ˜ ë˜ëŠ” ë‹¤ë³€ìˆ˜ë¡œ ë°”ê¿” ë‚˜íƒ€ë‚´ëŠ” ê²ƒì„ **[Change of Variable](https://en.wikipedia.org/wiki/Change_of_variables)**ì´ë¼ê³  í•©ë‹ˆë‹¤. 

ì˜ˆì‹œë¥¼ ë“¤ë©´, ì–´ë– í•œ ëœë¤ ë³€ìˆ˜ $x$ê°€ ìˆê³ , $x$ì— ëŒ€í•œ **í™•ë¥  ë°€ë„ í•¨ìˆ˜(Probability Density Function)**ê°€ ìˆë‹¤ê³  ê°€ì •í•˜ê² ìŠµë‹ˆë‹¤.

$x \sim p(x) \\
z\sim \pi(z)$

ì—¬ê¸°ì„œ **ë³€ìˆ˜ $z$ê°€ ë³€ìˆ˜ $x$ë¥¼ ì˜ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ì ì¬ ë³€ìˆ˜(latent variable)**ì´ê³  $z$ì˜ í™•ë¥  ë°€ë„ í•¨ìˆ˜ $\pi(z)$ë¥¼ ì•Œê³  ìˆë‹¤ê³  ê°€ì •í•˜ê³ , **invertibleí•œ** ì¼ëŒ€ì¼ ëŒ€ì‘ í•¨ìˆ˜ $f$ë¥¼ ì‚¬ìš©í•´ì„œ, $x = f(z)$ë¡œ í‘œí˜„ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ê·¸ë¦¬ê³  **í•¨ìˆ˜ $f$ëŠ” invertible**í•˜ë‹¤ê³  ê°€ì •í–ˆê¸° ë•Œë¬¸ì—, $z=f^{-1}{(x)}$ë¡œ í‘œí˜„ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ì—¬ê¸°ì„œ ì €í¬ê°€ í•˜ê³  ì‹¶ì€ ê²ƒì€ $x$ì˜ unknown í™•ë¥  ë¶„í¬ì¸  $p(x)$ë¥¼ êµ¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

í™•ë¥  ë¶„í¬ì˜ ì •ì˜ë¥¼ í†µí•´,  $\int p(x)dx = \int \pi(z)dz = 1$ ë¡œ í‘œí˜„ì´ ê°€ëŠ¥í•˜ê³  

**Change of Variable**ì„ ì ìš©í•˜ë©´, $\int p(x)dx = \int \pi(f^{-1}(x))d f^{-1}(x)$ ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

ì´ê²ƒì„ $p(x) = \pi (z)\Bigl|\frac{dz}{dx}\Bigl| = \pi(f^{-1}(x))\Bigl|\frac{df^{-1}}{dx}\Bigl| = \pi(f^{-1}(x))\Bigl|(f^{-1})'(x)\Bigl|$ ì™€ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆê³  êµ¬í•˜ê³  ì‹¶ì—ˆë˜ $**p(x)$ë¥¼ $z$ì˜ í™•ë¥ ë°€ë„í•¨ìˆ˜ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤**.

ì‹¤ì œë¡œëŠ” ê³ ì°¨ì›ì˜ ë³€ìˆ˜ë“¤ì„ ë‹¤ë£¨ê¸° ë•Œë¬¸ì—, ì´ë¥¼ ë‹¤ë³€ìˆ˜(Multi-variable)ê´€ì ìœ¼ë¡œ ë‹¤ì‹œ í‘œí˜„ì„ í•˜ê¸° ìœ„í•´ í–‰ë ¬ì„ ì‚¬ìš©í•˜ë©´,

$\mathbf{z} \sim \pi(\mathbf{z}), \mathbf{x} = f(\mathbf{z}), \mathbf{z} = f^{-1}(\mathbf{x})\\
p(\mathbf{x})=\pi(\mathbf{z})\Bigl|\text{det}\frac{d\mathbf{z}}{d\mathbf{x}}\Bigl| = \pi(f^{-1}(\mathbf{x}))\Bigl|\text{det}\frac{df^{-1}}{d\mathbf{x}}\Bigl|$

ì™€ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## Jacobian Matrix and Determinant

![Untitled](assets/img/2023-11-16-write-Glow/fig4.png)

**[Jacobian matrix](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant)**ëŠ” ìœ„ì™€ê°™ì´ ë²¡í„° $\text{x}$, $\text{y}$ì— ëŒ€í•œ ì¼ì°¨ í¸ë¯¸ë¶„ì„ í–‰ë ¬ë¡œ ë‚˜íƒ€ë‚¸ ê²ƒì…ë‹ˆë‹¤.

ì¦‰, ìš°ë¦¬ê°€ $n$ì°¨ì› ì…ë ¥ ë²¡í„° $\text{x}$ë¥¼ $m$ì°¨ì› ì¶œë ¥ ë²¡í„° $\text{y}$ë¡œ mappingí•˜ëŠ” ($\text{y}:\mathbb{R}^n \mapsto \mathbb{R}^m$)í•¨ìˆ˜ê°€ ì£¼ì–´ì§€ë©´ ì´ í•¨ìˆ˜ì˜ ëª¨ë“  1ì°¨ í¸ë¯¸ë¶„ í•¨ìˆ˜ í–‰ë ¬ì„ ì´ë ‡ê²Œ Jacobian matrixë¡œ ê°„ë‹¨í•˜ê²Œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![Untitled](assets/img/2023-11-16-write-Glow/fig5.png)

![Untitled](assets/img/2023-11-16-write-Glow/fig6.png)

**[Determinant](https://en.wikipedia.org/wiki/Determinant)**ëŠ” í–‰ë ¬ì„ ëŒ€í‘œí•˜ëŠ” ê°’ìœ¼ë¡œ, ì •ë°©í–‰ë ¬(Square Matrix)ì— ì–´ë–¤ íŠ¹ì •í•œ ë°©ë²•ìœ¼ë¡œ í•˜ë‚˜ì˜ ìˆ˜ë¥¼ ëŒ€ì‘ì‹œí‚¤ëŠ” ì¼ì¢…ì˜ í•¨ìˆ˜ì…ë‹ˆë‹¤.

Determinantì˜ ì„±ì§ˆì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

(1) í–‰ë ¬Â $A$ì˜ ì„ì˜ì˜ í–‰ì— ìŠ¤ì¹¼ë¼ ê³±ì„ í•œ ë’¤ ë‹¤ë¥¸ í–‰ì— ë”í•´Â $B$ë¥¼ ë§Œë“¤ì—ˆì„ ë•Œ ë‘ í–‰ë ¬ì˜ determinantëŠ” ê°™ë‹¤.

(2) í–‰ë ¬Â $A$ì˜ ì„ì˜ì˜ í–‰ì„ ë‹¤ë¥¸ í–‰ê³¼ ë°”ê¾¸ì–´Â $B$ë¥¼ ë§Œë“¤ì—ˆì„ ë•ŒÂ ğ‘‘ğ‘’ğ‘¡$B$$=âˆ’$ğ‘‘ğ‘’ğ‘¡$A$

(3) í–‰ë ¬Â $A$ì˜ ì„ì˜ì˜ í–‰ì— ìŠ¤ì¹¼ë¼ ê³±ì„ í•´Â $B$ë¥¼ ë§Œë“¤ì—ˆì„ ë•ŒÂ ğ‘‘ğ‘’ğ‘¡$B$$=$$k$ğ‘‘ğ‘’ğ‘¡$A$

(4)Â **ì‚¼ê°í–‰ë ¬(triangular matrix)**ì˜ í–‰ë ¬ì‹ì€ ì£¼ ëŒ€ê°ì›ì†Œë“¤ì˜ ê³±ê³¼ ê°™ë‹¤.

(5) í–‰ë ¬Â $A$ê°€Â **ê°€ì—­(invertible)**ì„ê³¼Â ğ‘‘ğ‘’ğ‘¡$A$$â‰ 0$.

(6)Â ğ‘‘ğ‘’ğ‘¡$ğ´^T$$=$ğ‘‘ğ‘’ğ‘¡$A$

(7)Â ğ‘‘ğ‘’ğ‘¡$AB$$=$(ğ‘‘ğ‘’ğ‘¡$A$)(ğ‘‘ğ‘’ğ‘¡$B$)

# Flow-based Generative Models

## Normalizing Flows

**Normalizing Flow**ì˜ ë™ì‘ ê³¼ì •ì€ ê°„ë‹¨í•˜ê²Œ í‘œí˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

![Untitled](assets/img/2023-11-16-write-Glow/fig7.png)

($x$ëŠ” high dimensional data, $z$ëŠ” latent variable)

![Untitled](assets/img/2023-11-16-write-Glow/fig8.png)

ì—¬ê¸°ì„œ $**z$ì˜ í™•ë¥  ë¶„í¬ë¥¼ ì•Œê³  ìˆë‹¤ë©´ $x$ì˜ í™•ë¥  ë¶„í¬ë¥¼ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤**.

ì•„ê¹Œ Change of Variable Theorem ì„¤ëª…ì—ì„œì™€ ë§ˆì°¬ê°€ì§€ë¡œ, $x = f(z)$, $z=f^{-1}{(x)}$ì¸ ìƒí™©ì—ì„œ, 

$p(x)$ $=$ $p(z)detJ$ë¡œ $z$ì˜ í™•ë¥  ë¶„í¬ì— scalarê°’ì¸ determinantë¥¼ ê³±í•´ì„œ $x$ì˜ í™•ë¥  ë¶„í¬ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê·¸ë¦¬ê³  ì–‘ë³€ì— $log$ë¥¼ ì”Œìš°ë©´,  $log($$p(x))$ $=$ $log(p(z))+log(detJ)$ë¡œ í‘œí˜„ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ê·¸ëŸ°ë° ì‹¤ì œ ë°ì´í„°ì¸  $x$ëŠ” ë³´í†µ ë§¤ìš° ë³µì¡í•œ ë¶„í¬ë¥¼ ê°€ì§€ê¸° ë•Œë¬¸ì— $x$ì™€ $z$ë¥¼ í•˜ë‚˜ì˜ í•¨ìˆ˜ë¡œ ë°”ë¡œ ì—°ê²°í•˜ê¸°ëŠ” ì–´ë µìŠµë‹ˆë‹¤.

![Untitled](assets/img/2023-11-16-write-Glow/fig9.png)

$p(x)=p(z_{1})detJ_{1}$ 

$p(z_{1})=p(z_{2})detJ_{2}$ 

$p(z_{2})=p(z_{3})detJ_{3}$ 

â€¦.

$p(z_{n-1})=p(z_{n})detJ_{n}$ 

ê·¸ë˜ì„œ ì´ë ‡ê²Œ ë§ì€ í•¨ìˆ˜ë¥¼ í†µí•´ì„œ mappingì„ í•´ì£¼ëŠ” ê²ƒì´ê³  $p(x) = p(z_{n})\Pi_{n}(detJ_{n})$ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ìµœì¢…ì ìœ¼ë¡œ log likelihoodëŠ”  $log($$p(x))$ $=$ $log(p(z_{n}))+\Sigma_n log(detJ_{n})$ë¡œ í‘œí˜„ë©ë‹ˆë‹¤.

![Untitled](assets/img/2023-11-16-write-Glow/fig10.png)

ë”¥ëŸ¬ë‹ì—ì„œ Normalizing Flowë¥¼ ì ìš©í•˜ì—¬ $x$ì˜ í™•ë¥  ë¶„í¬ë¥¼ ì•Œê¸° ìœ„í•´ì„œëŠ” **2ê°€ì§€ ì¡°ê±´ì´ ê¼­ ì¶©ì¡±**ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

1. **í•¨ìˆ˜ $f$ì˜ ì—­í•¨ìˆ˜ê°€ ì¡´ì¬í•´ì•¼ í•¨** (**invertible** $f$)
2. **Jacobianì˜ Determinantë„ ê³„ì‚° ê°€ëŠ¥í•´ì•¼ í•¨**

ì´ 2ê°€ì§€ ì¡°ê±´ì„ ê³ ë ¤í•˜ì—¬ í•¨ìˆ˜  $f$ë¥¼ ì„ íƒí•´ì•¼ í•˜ê³ , ì´ flow í•¨ìˆ˜ë“¤ì„ ëª¨ë¸ë§ˆë‹¤ ì–´ë–»ê²Œ êµ¬í˜„í•˜ëŠ” ì§€ê°€ ë‹¤ë¦…ë‹ˆë‹¤.

# Proposed Generative Flow

![One step of our flow in Glow paper](assets/img/2023-11-16-write-Glow/fig11.png)

One step of our flow in Glow paper

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” NICE[Dinh et al., 2014]ì™€ RealNVP[Dinh et al., 2016]ì˜ flowë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ generative flowì¸ Glow ëª¨ë¸ì„ ì œì•ˆí•˜ê³  ìˆìŠµë‹ˆë‹¤.

ìœ„ì˜ ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, one flow stepì€ **actnorm**, **invertible 1 x 1 convolution**, **affine coupling layer**ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.

![Multi-scale architecture (Dinh et al., 2016)](assets/img/2023-11-16-write-Glow/fig12.png)

Multi-scale architecture (Dinh et al., 2016)

ì´ë ‡ê²Œ êµ¬ì„±ëœ flow stepì€ ìœ„ì˜ ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, RealNVPì—ì„œ ì œì•ˆëœ multi-scale architecture êµ¬ì¡°ì—ì„œ í™œìš©ë©ë‹ˆë‹¤

ì´ êµ¬ì¡°ëŠ” squeezing operationì„ í†µí•´ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Â 

![Untitled](assets/img/2023-11-16-write-Glow/fig13.png)

ì´ë¯¸ì§€ë¥¼ sub-squareë¡œ reshapeí•˜ëŠ”ë°(4X4X1 ->2X2X4),Â ì´ ë°©ë²•ì€ spatial sizeë¥¼ ì±„ë„ì˜ ìˆ˜ë¡œ íš¨ê³¼ì ìœ¼ë¡œ tradeí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ê·¸ë˜ì„œ ì „ì²´ì ì¸ ê³¼ì •ì„ ë³´ë©´, xë¼ëŠ” ì´ë¯¸ì§€ë¥¼ ì…ë ¥ê°’ìœ¼ë¡œ ë„£ê³ Â squeezeë¥¼ í†µí•´ ê³µê°„í•´ìƒë„ë¥¼ ì¤„ì…ë‹ˆë‹¤.

ê·¸ë¦¬ê³  flow stepì„ Kë²ˆ ë°˜ë³µí•œ í›„, splitì„ ì§„í–‰í•˜ê³  ì´ë¥¼ L-1ë²ˆ ë°˜ë³µí•˜ëŠ” multi-scale êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤.Â 

ê²°ë¡ ì ìœ¼ë¡œ flow step Kë²ˆì„ Lë²ˆ ë°˜ë³µí•˜ëŠ” ê²ƒì´ ë©ë‹ˆë‹¤.

## Actnorm: scale and bias layer with data dependent initialization

![Untitled](assets/img/2023-11-16-write-Glow/fig14.png)

Actnormì€ Activation Outputì— **Affine Transformation**ì„ ì ìš©í•˜ëŠ”ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ Batch Normalizationê³¼ ìœ ì‚¬í•œ ì—­í• ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì¸ë°Â invertibleí•˜ê³  Log Determinant ê³„ì‚°ì´ ì‰¬ì›Œ Normalizing Flowì— ì ìš©í•˜ê¸° ìˆ˜ì›”í•©ë‹ˆë‹¤.

**RealNVP**ì—ì„œëŠ” batch normalizationì„ í™œìš©í–ˆì§€ë§Œ, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” **activation normalization** (**actnorm**) layerë¥¼ ëŒ€ì‹  ì œì•ˆí•˜ê³  ìˆìŠµë‹ˆë‹¤.

ê¸°ì¡´ì— ì‚¬ìš©í–ˆë˜ Batch normalizationì—ì„œëŠ” mini-batch sizeê°€ ì‘ì„ ë•Œ activationì˜ noise variationì´ ì»¤ì ¸ì„œ batch normalizationì˜ ì„±ëŠ¥ì´ ë–¨ì–´ì§„ë‹¤ëŠ” ë‹¨ì ì´ ìˆì—ˆìŠµë‹ˆë‹¤.

í° ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œëŠ” GPU ì„±ëŠ¥ í•œê³„ë•Œë¬¸ì— mini-batch sizeë¥¼ ì¤„ì—¬ì•¼ í•œë‹¤ëŠ” ë¬¸ì œì ì´ ìˆì—ˆê³  ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ê° channelë§ˆë‹¤ scaleê³¼ bias parameterë¥¼ ì´ìš©í•´ì„œ **affine transformation of the activation**ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ì´ëŸ¬í•œ ê³¼ì •ì„ ê±°ì³ì„œ ì´ˆê¸°ì˜ mini-batch ë°ì´í„°ì— ëŒ€í•´ channel ë³„ë¡œ **zero mean**ê³¼ **unit variation**ì„ ê°€ì§€ë„ë¡ ì´ˆê¸°í™”ë¥¼ ì‹œí‚µë‹ˆë‹¤.

ì´ë¥¼ ë³´í†µ **data dependent initialization[Salimans and Kingma, 2016]**ì´ë¼ê³  í•©ë‹ˆë‹¤.

initializationí›„ì— scaleê³¼ bias parameterëŠ” ë°ì´í„°ì™€ independentí•œ regular trainable parameterë¡œì„œ ë‹¤ë£¨ì–´ì§‘ë‹ˆë‹¤.

## **Invertible 1 x 1 convolution**

![Untitled](assets/img/2023-11-16-write-Glow/fig15.png)

Invertible 1 x 1 convolutionì€ Coupling Layerì˜ Inputì„ Split í•˜ëŠ” ìš©ë„ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. 

1Ã—1 Convolutionì„ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ê³„ì‚° ë³µì¡ë„ê°€ í¬ì§€ ì•Šìœ¼ë©´ì„œë„ ì‰½ê²Œ Log Determinantë¥¼ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

RealNVPì—ì„œëŠ” channelì˜ ìˆœì„œë¥¼ ë°˜ëŒ€ë¡œ ë°”ê¾¸ëŠ” permutationì´ í¬í•¨ëœ flowë¥¼ ì œì•ˆí–ˆì§€ë§Œ,

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì•ì„œ ì œì•ˆëœ fixed permutation ëŒ€ì‹ ì— (learned) invertible 1 x 1 convolutionì„ ì œì•ˆí•˜ê³  ìˆìŠµë‹ˆë‹¤.

## Affine Coupling Layers

![NICEì˜ Coupling Layer](assets/img/2023-11-16-write-Glow/fig16.png)

NICEì˜ Coupling Layer

![RealNVPì˜ Affine Transformation](assets/img/2023-11-16-write-Glow/fig17.png)

RealNVPì˜ Affine Transformation

Coupling Layerë€ Inputì„ ë‘˜ë¡œ ë‚˜ëˆ  êµ¬ì„±í•œ Matrixì¸ë°, RealNVPì—ì„œ ì œì•ˆëœ **Affine Coupling layer**ë¥¼ ë³¸ ë…¼ë¬¸ì—ì„œë„ í™œìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.

![Glowì˜ affine coupling layer](assets/img/2023-11-16-write-Glow/fig18.png)

Glowì˜ affine coupling layer

ë‹¤ë§Œ, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ê¸°ì¡´ì˜ ë°©ì‹ê³¼ ì„¸ ê°€ì§€ê°€ í¬ì¸íŠ¸ê°€ ë‹¤ë¦…ë‹ˆë‹¤.

### Zero initialization

í•™ìŠµì„ ì‹œì‘í•  ë•Œ Affine Coupling Layerê°€ Identity Functionì´ ë˜ë„ë¡ ë§ˆì§€ë§‰ convolution layerë¥¼ Zeroë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. ì´ëŠ” ë§¤ìš° ê¹Šì€ ëª¨ë¸ì—ì„œì˜ í•™ìŠµì„ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤.

### Split and concatenation

ì±„ë„ì„ ë‚˜ëˆ„ëŠ” ë°©ì‹ìœ¼ë¡œÂ NICEì™€ ê°™ì´ channelì„ ë”°ë¼ ì ˆë°˜ì„ ë‚˜ëˆ„ëŠ” ë°©ë²•ì´ ìˆê³ Â RealNVPì²˜ëŸ¼ checkerboard patternìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤. GlowëŠ” NICE ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” channel dimensionì—ì„œë§Œ splitì„ ìˆ˜í–‰í•˜ë©´ì„œ ì „ì²´ ì•„í‚¤í…ì³ë¥¼ ê°„ì†Œí™”í•˜ê³  ìˆìŠµë‹ˆë‹¤.

### Permutation

NICEëŠ” Channelì˜ ìˆœì„œë¥¼ ë°˜ëŒ€ë¡œ(reversely) ë°”ê¾¸ëŠ” ë°©ë²•ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. 

RealNVPëŠ” Fixed Random Permutation ë°©ì‹ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. 

ë³¸ ë…¼ë¬¸ì˜ GlowëŠ” 1Ã—1 convolution ë°©ì‹ì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.

![The three main components of our proposed flow, their reverses, and their log-determinants in Glow paper](assets/img/2023-11-16-write-Glow/fig19.png)

The three main components of our proposed flow, their reverses, and their log-determinants in Glow paper

# Experiments

ë¨¼ì € Quantitative Experiment ê²°ê³¼ë“¤ì„ ë³´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

## **Gains using invertible** 1 Ã— 1 **Convolution**

![Comparison of the three variants - a reversing operation as described in the RealNVP, a fixed random permutation, and our proposed invertible 1 Ã— 1 convolution, with additive (left) versus affine (right) coupling layers ](assets/img/2023-11-16-write-Glow/fig20.png)

Comparison of the three variants - a reversing operation as described in the RealNVP, a fixed random permutation, and our proposed invertible 1 Ã— 1 convolution, with additive (left) versus affine (right) coupling layers 

Reverse, Shuffle, 1Ã—1 convolution ë°©ì‹ì˜ Permutationì„ ì‹¤í—˜í•œ ê²°ê³¼ì…ë‹ˆë‹¤. 

ê·¸ë˜í”„ë¥¼ ë³´ë©´ ë‹¤ì–‘í•œ Permutation ë°©ë²•ë“¤ ì¤‘ 1Ã—1 convolution ë°©ì‹ì˜ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì€ ëª¨ìŠµì…ë‹ˆë‹¤.

## **Comparison with RealNVP on standard benchmarks**

![Untitled](assets/img/2023-11-16-write-Glow/fig21.png)

ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì— ëŒ€í•œ RealNVP ëª¨ë¸ê³¼ì˜ ë¹„êµì…ë‹ˆë‹¤. Bits per Dimension ì„±ëŠ¥ì„ ì¸¡ì •í–ˆìŠµë‹ˆë‹¤.

GLOWëŠ” ëª¨ë“  ë°ì´í„°ì…‹ì—ì„œ RealNVP ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤.

ì´ì œ Qualitative Experiment ê²°ê³¼ë“¤ì„ ë³´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

Glow ëª¨ë¸ì´ **ê³ í•´ìƒë„(high resolutions)ê¹Œì§€ scalingì´ ê°€ëŠ¥í•œì§€**, **í˜„ì‹¤ì ì¸ ìƒ˜í”Œë“¤ì„ ë§Œë“œëŠ”ì§€**, **meaningful latent spaceë¥¼ ë§Œë“¤ê³  ìˆëŠ”ì§€**ë¥¼ ì¤‘ì ìœ¼ë¡œ ë´ì•¼ í•©ë‹ˆë‹¤.

### Synthesis

![ Random samples from the model, with temperature 0.7](assets/img/2023-11-16-write-Glow/fig22.png)

 Random samples from the model, with temperature 0.7

Glow ëª¨ë¸ì—ì„œ ëœë¤í•˜ê²Œ ìƒ˜í”Œë§í•œ ì´ë¯¸ì§€ë“¤ì¸ë°, ì´ë¯¸ì§€ì˜ í’ˆì§ˆì´ ê½¤ ì¢‹ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Interpolation

![Linear interpolation in latent space between real images](assets/img/2023-11-16-write-Glow/fig23.png)

Linear interpolation in latent space between real images

latent spaceìƒì—ì„œ 2ê°œì˜ ì‹¤ì œ ë°ì´í„°ì˜ encodingí•œ ë²¡í„°ê°„ linear interpolationì„ í–ˆê³ , ê·¸ interpolation latentë¡œë¶€í„° ìƒ˜í”Œë§í•œ ì´ë¯¸ì§€ë“¤ì…ë‹ˆë‹¤.

ê½¤ í˜„ì‹¤ì ìœ¼ë¡œ ì‹¤ì œ ë‘ ì´ë¯¸ì§€ ì‚¬ì´ì˜ ì¤‘ê°„ ì–¼êµ´ ì´ë¯¸ì§€ë¥¼ ë³´ì—¬ì£¼ê³  ìˆëŠ” ê²ƒì„ ë³´ì•„, Glow ëª¨ë¸ì´ meaningful latent spaceë¥¼ ì˜ ë§Œë“¤ê³  ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Semantic Manipulation of attributes of a face

![Smiling](assets/img/2023-11-16-write-Glow/fig24.png)

Smiling

![Pale Skin](assets/img/2023-11-16-write-Glow/fig25.png)

Pale Skin

![Blond Hair](assets/img/2023-11-16-write-Glow/fig26.png)

Blond Hair

![Young](assets/img/2023-11-16-write-Glow/fig27.png)

Young

![Narrow Eyes](assets/img/2023-11-16-write-Glow/fig28.png)

Narrow Eyes

![Male](assets/img/2023-11-16-write-Glow/fig29.png)

Male

ë‹¤ìŒì€ ë¯¸ì†Œ, í”¼ë¶€ìƒ‰ ë“±ì˜ Semantic Maniplation ì‹¤í—˜ì…ë‹ˆë‹¤. 

CelebA ë°ì´í„°ì…‹ì—ëŠ” Smiling, Blond Hair ë“±ì˜ Labelì´ ì¡´ì¬í•©ë‹ˆë‹¤. 

ì¼ë‹¨ Labeling ì—†ì´ Normalizing Flowë¥¼ í•™ìŠµí•œ ë‹¤ìŒ True/False Label ë°ì´í„°ë“¤ì˜ Average(z)ë¥¼ ê°ê° êµ¬í•©ë‹ˆë‹¤. ê·¸ ë‹¤ìŒ ë‘ ê°œì˜ ì ì—ì„œ Interpolationì„ ìˆ˜í–‰í•˜ë©° Imageë¥¼ ìƒì„±í•´ëƒ…ë‹ˆë‹¤.

### Effect of model depth

![Samples from shallow model on left vs deep model on right. Shallow model has L = 4
levels, while deep model has L = 6 levels](assets/img/2023-11-16-write-Glow/fig30.png)

Samples from shallow model on left vs deep model on right. Shallow model has L = 4
levels, while deep model has L = 6 levels

ëª¨ë¸ì´ ì ë‹¹íˆ ê¹Šì„ë•Œ Long Range Dependencyë¥¼ ì˜ í•™ìŠµí•˜ëŠ” ëª¨ìŠµì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Effect of temperature

![Effect of change of temperature. From left to right, samples obtained at temperatures
0, 0.25, 0.6, 0.7, 0.8, 0.9, 1.0](assets/img/2023-11-16-write-Glow/fig31.png)

Effect of change of temperature. From left to right, samples obtained at temperatures
0, 0.25, 0.6, 0.7, 0.8, 0.9, 1.0

Temperatureê°€ ë†’ìœ¼ë©´ noisyí•œ ëª¨ìŠµì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” 0.7ì¼ë•Œ ê°€ì¥ ì ì ˆí•œ ê²°ê³¼ë¥¼ ì–»ì—ˆë‹¤ê³  ì–¸ê¸‰í•˜ê³  ìˆìŠµë‹ˆë‹¤.

# References

D. P. Kingma and P. Dhariwal ,"[Glow](https://proceedings.neurips.cc/paper_files/paper/2018/hash/d139db6a236200b21cc7f752979132d0-Abstract.html): Generative flow with invertible 1x1 convolutions,â€ *NeurIPS*, 2018.

A. Oord et al., â€œ[Wavenet](https://arxiv.org/abs/1609.03499): A generative model for raw audio,â€ *arXiv*, 2016

Dinh, Laurent, David Krueger, and Yoshua Bengio ,"[Nice](https://arxiv.org/abs/1410.8516): Non-linear independent components estimation,â€ *arXiv*, 2014.

Dinh, Laurent, Jascha Sohl-Dickstein, and Samy Bengio ,â€œ[DENSITY ESTIMATION USING REAL NVP](https://arxiv.org/abs/1605.08803)â€, *NeurlPS*, 2016.

Gomez et al., "[The reversible residual network: Backpropagation without storing activations](https://proceedings.neurips.cc/paper_files/paper/2017/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.html),"Â *NeurlPS*, 2017.